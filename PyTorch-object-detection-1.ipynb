{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with PyTorch and Mask R-CNN \n",
    "\n",
    "In this tutorial, you will finetune a pre-trained [Mask R-CNN](https://arxiv.org/abs/1703.06870) model on images from the [Penn-Fudan Database for Pedestrian Detection and Segmentation](https://www.cis.upenn.edu/~jshi/ped_html/). The dataset has 170 images with 345 instances of pedestrians.\n",
    "\n",
    "## Prerequisities\n",
    "\n",
    "- If you are using an Azure Machine Learning Notebook VM, your environment already meets these prerequisites. Otherwise, go through the [Configuration](https://docs.microsoft.com/azure/machine-learning/how-to-configure-environment) steps to install the Azure Machine Learning Python SDK and [create an Azure ML Workspace](https://docs.microsoft.com/azure/machine-learning/how-to-manage-workspace#create-a-workspace).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.65\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics\n",
    "\n",
    "Opt-in diagnostics for better experience, quality, and security in future releases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning diagnostics collection on. \n"
     ]
    }
   ],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize a workspace\n",
    "\n",
    "Initialize a [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) object from the existing workspace you created in the Prerequisites step. `Workspace.from_config()` creates a workspace object from the details stored in `config.json`, using the [from_config()](https://docs.microsoft.com/python/api/azureml-core/azureml.core.workspace(class)?view=azure-ml-py#from-config-path-none--auth-none---logger-none---file-name-none-) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: gopalv-ws\n",
      "Azure region: westus2\n",
      "Subscription id: 15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\n",
      "Resource group: aifxdemo\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or attach existing Azure ML Managed Compute\n",
    "\n",
    "You will need to create a [compute target](https://docs.microsoft.com/azure/machine-learning/concept-compute-target) for training your model. In this tutorial, we use [Azure ML managed compute](https://docs.microsoft.com/azure/machine-learning/how-to-set-up-training-targets#amlcompute) for our remote training compute resource. Specifically, the below code creates a `STANDARD_NC6` GPU cluster that autoscales from 0 to 4 nodes.\n",
    "\n",
    "**Creation of Compute takes approximately 5 minutes.** If the Aauzre ML Compute with that name is already in your workspace, this code will skip the creation process. \n",
    "\n",
    "As with other Azure servies, there are limits on certain resources associated with the Azure Machine Learning service. Please read [this article](https://docs.microsoft.com/azure/machine-learning/how-to-manage-quotas) on the default limits and how to request more quota.\n",
    "\n",
    "> Note that the below code creates GPU compute. If you instead want to create CPU compute, provide a different VM size to the `vm_size` parameter, such as `STANDARD_D2_V2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 1, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2020-03-03T18:59:34.532000+00:00', 'errors': None, 'creationTime': '2020-02-14T22:38:17.474022+00:00', 'modifiedTime': '2020-02-14T22:39:03.552424+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster1\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on the remote compute\n",
    "\n",
    "### Create a project directory\n",
    "Create a directory that will contain all the code from your local machine that you will need access to on the remote resource. This includes the training script an any additional files your training script depends on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './pytorch-peds'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibly helpful: [this link](https://github.com/drabastomek/GTC/blob/master/SJ_2020/workshop/1_Setup/Setup.ipynb), and this sample dockerfile from Jordan:\n",
    "\n",
    "```\n",
    "FROM mcr.microsoft.com/azureml/base-gpu:intelmpi2018.3-cuda9.0-cudnn7-ubuntu16.04\n",
    "\n",
    "# Install Horovod, temporarily using CUDA stubsddd \n",
    "RUN ldconfig /usr/local/cuda/lib64/stubs && \\     \n",
    "# Install AzureML SDK     \n",
    "pip install --no-cache-dir azureml-defaults && \\     \n",
    "# Install PyTorch     \n",
    "pip install --no-cache-dir tensorflow==2.0.0b1 tensorflow-gpu==2.0.0b1 keras==2.0.8 matplotlib==3.0.3 seaborn==0.9.0 requests==2.21.0 bs4==0.0.1 imageio==2.5.0 sklearn pandas==0.24.2 numpy==1.16.2 hickle==3.4.3 && \\     \n",
    "# Install Horovod     \n",
    "pip install --no-cache-dir horovod==0.13.5 && \\     ldconfig\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy training script and dependencies into project directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./pytorch-peds/script.py'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('data.py', project_folder)\n",
    "shutil.copy('model.py', project_folder)\n",
    "shutil.copy('script.py', project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vision' already exists and is not an empty directory.\n",
      "error: pathspec 'v0.3.0' did not match any file(s) known to git.\n",
      "/home/gopalv/pytorch-object/vision\n",
      "/home/gopalv/pytorch-object\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pytorch/vision.git\n",
    "\n",
    "!git checkout v0.3.0\n",
    "\n",
    "%cd vision\n",
    "!cp references/detection/utils.py ../\n",
    "!cp references/detection/transforms.py ../\n",
    "!cp references/detection/coco_eval.py ../\n",
    "!cp references/detection/engine.py ../\n",
    "!cp references/detection/coco_utils.py ../\n",
    "%cd ..\n",
    "\n",
    "files_to_copy = ['utils', 'transforms', 'coco_eval', 'engine', 'coco_utils']\n",
    "for file in files_to_copy:\n",
    "    shutil.copy('./vision/references/detection/'+ file + '.py', project_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data and upload to Azure blob storage\n",
    "\n",
    "First we download the sample dataset, and extract the images into local storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation  PNGImages  PedMasks  added-object-list.txt\treadme.txt\r\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "data_file = './test.zip'\n",
    "\n",
    "urllib.request.urlretrieve('https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip', data_file)\n",
    "zip = ZipFile(file=data_file)\n",
    "zip.extractall()\n",
    "!ls PennFudanPed/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we upload the data files to the datastore associated with this workspace, so that we can access them during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspaceblobstore AzureBlob gopalvws3790775563 azureml-blobstore-e47496c6-9688-4277-a05b-ceb722514b9d\n",
      "Uploading an estimated of 512 files\n",
      "Target already exists. Skipping upload for data/added-object-list.txt\n",
      "Target already exists. Skipping upload for data/readme.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00001.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00002.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00003.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00004.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00005.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00006.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00007.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00008.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00009.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00010.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00011.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00012.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00013.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00014.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00015.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00016.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00017.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00018.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00019.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00020.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00021.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00022.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00023.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00024.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00025.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00026.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00027.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00028.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00029.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00030.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00031.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00032.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00033.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00034.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00035.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00036.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00037.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00038.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00039.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00040.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00041.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00042.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00043.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00044.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00045.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00046.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00047.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00048.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00049.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00050.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00051.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00052.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00053.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00054.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00055.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00056.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00057.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00058.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00059.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00060.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00061.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00062.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00063.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00064.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00065.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00066.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00067.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00068.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00069.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00070.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00071.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00072.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00073.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00074.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00001.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00002.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00003.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00004.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00005.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00006.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00007.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00008.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00009.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00010.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00011.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00012.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00013.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00014.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00015.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00016.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00017.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00018.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00019.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00020.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00021.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00022.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00023.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00024.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00025.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00026.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00027.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00028.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00029.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00030.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00031.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00032.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00033.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00034.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00035.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00036.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00037.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00038.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00039.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00040.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00041.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00042.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00043.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00044.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for data/Annotation/PennPed00045.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00046.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00047.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00048.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00049.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00050.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00051.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00052.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00053.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00054.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00055.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00056.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00057.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00058.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00059.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00060.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00061.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00062.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00063.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00064.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00065.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00066.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00067.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00068.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00069.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00070.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00071.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00072.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00073.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00074.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00075.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00076.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00077.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00078.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00079.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00080.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00081.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00082.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00083.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00084.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00085.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00086.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00087.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00088.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00089.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00090.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00091.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00092.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00093.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00094.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00095.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00096.txt\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00001.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00002.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00003.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00004.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00005.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00006.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00007.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00008.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00009.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00010.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00011.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00012.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00013.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00014.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00015.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00016.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00017.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00018.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00019.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00020.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00021.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00022.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00023.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00024.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00025.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00026.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00027.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00028.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00029.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00030.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00031.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00032.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00033.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00034.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00035.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00036.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00037.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00038.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00039.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00040.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00041.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00042.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00043.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00044.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00045.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00046.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00047.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00048.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00049.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00050.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00051.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00052.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00053.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00054.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00055.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00056.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00057.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00058.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00059.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00060.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00061.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00062.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00063.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00064.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00065.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00066.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00067.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00068.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00069.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00070.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00071.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00072.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00073.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00074.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00001.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00002.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00003.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00004.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00005.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00006.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00007.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00008.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00009.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00010.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00011.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00012.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00013.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00014.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00015.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00016.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00017.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00018.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00019.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00020.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00021.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00022.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00023.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00024.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00025.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00026.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00027.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00028.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00029.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00030.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00031.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00032.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00033.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00034.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00035.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00036.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00037.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00038.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00039.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00040.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00041.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00042.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00043.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00044.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00045.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00046.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00047.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00048.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00049.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00050.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00051.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00052.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00053.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00054.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00055.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00056.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00057.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00058.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00059.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00060.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00061.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00062.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00063.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00064.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00065.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00066.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00067.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00068.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00069.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00070.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00071.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00072.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00073.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00074.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00075.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00076.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00077.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00078.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00079.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00080.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00081.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00082.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00083.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00084.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00085.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00086.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00087.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00088.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00089.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00090.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00091.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00092.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00093.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00094.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00095.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00096.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00001_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00002_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00003_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00004_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00005_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00006_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00007_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00008_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00009_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00010_mask.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00011_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00012_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00013_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00014_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00015_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00016_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00017_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00018_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00019_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00020_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00021_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00022_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00023_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00024_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00025_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00026_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00027_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00028_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00029_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00030_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00031_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00032_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00033_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00034_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00035_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00036_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00037_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00038_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00039_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00040_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00041_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00042_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00043_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00044_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00045_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00046_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00047_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00048_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00049_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00050_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00051_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00052_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00053_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00054_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00055_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00056_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00057_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00058_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00059_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00060_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00061_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00062_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00063_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00064_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00065_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00066_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00067_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00068_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00069_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00070_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00071_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00072_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00073_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00074_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00001_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00002_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00003_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00004_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00005_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00006_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00007_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00008_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00009_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00010_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00011_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00012_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00013_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00014_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00015_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00016_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00017_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00018_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00019_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00020_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00021_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00022_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00023_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00024_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00025_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00026_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00027_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00028_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00029_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00030_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00031_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00032_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00033_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00034_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00035_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00036_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00037_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00038_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00039_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00040_mask.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for data/PedMasks/PennPed00041_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00042_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00043_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00044_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00045_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00046_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00047_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00048_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00049_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00050_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00051_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00052_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00053_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00054_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00055_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00056_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00057_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00058_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00059_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00060_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00061_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00062_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00063_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00064_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00065_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00066_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00067_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00068_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00069_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00070_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00071_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00072_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00073_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00074_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00075_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00076_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00077_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00078_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00079_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00080_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00081_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00082_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00083_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00084_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00085_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00086_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00087_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00088_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00089_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00090_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00091_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00092_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00093_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00094_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00095_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00096_mask.png\n",
      "Uploaded 0 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_4b3b9306386f4d139bd6f350e3e4f96c"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)\n",
    "\n",
    "ds.upload('./PennFudanPed', target_path='data', overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'data')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"name\": \"penn_ds\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"Penn Fudan pedestrian data\",\n",
       "    \"workspace\": \"Workspace.create(name='gopalv-ws', subscription_id='15ae9cb6-95c1-483d-a0e3-b1a1a3b06324', resource_group='aifxdemo')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "datastore_paths = [(ds, 'data')]\n",
    "penn_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "penn_ds.register(workspace=ws,\n",
    "                 name='penn_ds',\n",
    "                 description='Penn Fudan pedestrian data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch-peds'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify dependencies with a custom Dockerfile\n",
    "\n",
    "There are a number of ways to [use environments](https://docs.microsoft.com/azure/machine-learning/how-to-use-environments) for specifying dependencies during model training. In this case, we use a custom Dockerfile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n",
      "WARNING - 'gpu_support' is no longer necessary; AzureML now automatically detects and uses nvidia docker extension when it is available. It will be removed in a future release.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "my_env = Environment(name='maskr-docker')\n",
    "my_env.docker.enabled = True\n",
    "with open(\"dockerfiles/Dockerfile1\", \"r\") as f:\n",
    "    dockerfile_contents_of_your_base_image=f.read()\n",
    "my_env.docker.base_dockerfile=dockerfile_contents_of_your_base_image \n",
    "my_env.docker.base_image = None\n",
    "my_env.docker.gpu_support = True\n",
    "my_env.python.interpreter_path = '/opt/miniconda/bin/python'\n",
    "my_env.python.user_managed_dependencies = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a ScriptRunConfig\n",
    "\n",
    "Use the [ScriptRunConfig](https://docs.microsoft.com/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py) class to define your run. Specify the source driectory, compute target, and environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: pytorch-peds,\n",
      "Id: pytorch-peds_1583273344_396fd37b,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Starting)\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "# follow pattern from here: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments#use-environments-for-training\n",
    "\n",
    "# Add training script to run config\n",
    "runconfig = ScriptRunConfig(source_directory=project_folder, script=\"script.py\")\n",
    "\n",
    "# Attach compute target to run config\n",
    "runconfig.run_config.target = cluster_name\n",
    "\n",
    "# Uncomment the line below if you want to try this locally first\n",
    "#runconfig.run_config.target = \"local\"\n",
    "\n",
    "# Attach environment to run config\n",
    "runconfig.run_config.environment = my_env\n",
    "\n",
    "# Submit run \n",
    "run = experiment.submit(runconfig)\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'runId': 'pytorch-peds_1583273344_396fd37b', 'target': 'gpu-cluster1', 'status': 'Starting', 'properties': {'_azureml.ComputeTargetType': 'amlcompute', 'ContentSnapshotId': '8fc97d08-d181-47c7-a5c5-03e4629c3414', 'azureml.git.repository_uri': 'git@github.com:gvashishtha/pytorch-object.git', 'mlflow.source.git.repoURL': 'git@github.com:gvashishtha/pytorch-object.git', 'azureml.git.branch': 'docker', 'mlflow.source.git.branch': 'docker', 'azureml.git.commit': '656e64ba234a71bb7ce564c406d0ce5155bb3df3', 'mlflow.source.git.commit': '656e64ba234a71bb7ce564c406d0ce5155bb3df3', 'azureml.git.dirty': 'False'}, 'inputDatasets': [], 'runDefinition': {'script': 'script.py', 'useAbsolutePath': False, 'arguments': [], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'gpu-cluster1', 'dataReferences': {}, 'data': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'environment': {'name': 'maskr-docker', 'version': 'Autosave_2020-02-29T00:36:31Z_cd1e72fb', 'python': {'interpreterPath': '/opt/miniconda/bin/python', 'userManagedDependencies': True, 'condaDependencies': {'name': 'project_environment', 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}], 'channels': ['conda-forge']}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': None, 'baseDockerfile': '# From https://github.com/microsoft/AzureML-BERT/blob/master/finetune/PyTorch/dockerfile\\n\\nFROM mcr.microsoft.com/azureml/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04\\n\\nRUN apt update && apt install git -y && rm -rf /var/lib/apt/lists/*\\n\\nRUN /opt/miniconda/bin/conda update -n base -c defaults conda\\nRUN /opt/miniconda/bin/conda install -y cython=0.29.15 numpy=1.18.1\\nRUN /opt/miniconda/bin/conda install -y pytorch=1.4 torchvision=0.5.0 -c pytorch\\n\\n# Install cocoapi, required for drawing bounding boxes\\nRUN git clone https://github.com/cocodataset/cocoapi.git && cd cocoapi/PythonAPI && python setup.py build_ext install\\n\\nRUN pip install azureml-defaults\\nRUN pip install \"azureml-dataprep[fuse]\"\\nRUN pip install pandas pyarrow\\n', 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}}, 'logFiles': {}}\n"
     ]
    }
   ],
   "source": [
    "# to get more details of your run\n",
    "print(run.get_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad11d4a437e34659bf21dc1af9b2d706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: pytorch-peds_1583273344_396fd37b\n",
      "Web View: https://mlworkspace.azure.ai/portal/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/aifxdemo/providers/Microsoft.MachineLearningServices/workspaces/gopalv-ws/experiments/pytorch-peds/runs/pytorch-peds_1583273344_396fd37b\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_7c329628f3ed87cd506b51138be56729b6a1aa785f383b471d3fdc7d9f6fd2c8_d.txt\n",
      "========================================================================================================================\n",
      "\n",
      "2020-03-03T22:10:13Z Starting output-watcher...\n",
      "2020-03-03T22:10:13Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_9125fd9b495cfdec8f7bf56c6d28d91d\n",
      "Digest: sha256:9d52d6d75a74e3891366cc9505dd2bd6e6e1e68766b6167933cce9bd8cbb823a\n",
      "Status: Image is up to date for gopalvwsdebb3b12.azurecr.io/azureml/azureml_9125fd9b495cfdec8f7bf56c6d28d91d:latest\n",
      "a468fb0b114d610d8be4447ff57a20f085b7efb165c2076bd4372d29a78c906f\n",
      "2020/03/03 22:10:20 Version: 3.0.01140.0002 Branch: merge Commit: 93399ed1\n",
      "2020/03/03 22:10:20 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2020/03/03 22:10:20 sshd runtime has already been installed in the container\n",
      "ssh-keygen: generating new host keys: DSA \n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_7c329628f3ed87cd506b51138be56729b6a1aa785f383b471d3fdc7d9f6fd2c8_d.txt\n",
      "===============================================================================================================\n",
      "\n",
      "Starting job preparation. Current time:2020-03-03T22:10:23.280832\n",
      "Extracting the control code.\n",
      "Creating directory: azureml-logs/\n",
      "Retrieving project from snapshot: 0afae1e0-a3e7-407e-b546-9b3a80ee9801\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 78\n",
      "Starting project file download.\n",
      "Finished project file download.\n",
      "Download from datastores if requested.\n",
      "Download or mount from datasets if requested.\n",
      "Job preparation is complete. Current time:2020-03-03T22:10:27.411228\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 126\n",
      "Entering Run History Context Manager.\n",
      "Preparing to call script [ script.py ] with arguments: []\n",
      "After variable expansion, calling script [ script.py ] with arguments: []\n",
      "\n",
      "Torch version: 1.4.0\n",
      "Using GPU\n",
      "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
      "\n",
      "  0%|          | 0.00/170M [00:00<?, ?B/s]\n",
      "  2%|▏         | 3.12M/170M [00:00<00:08, 21.0MB/s]\n",
      "  3%|▎         | 5.25M/170M [00:00<00:08, 20.9MB/s]\n",
      "  4%|▍         | 7.31M/170M [00:00<00:08, 21.1MB/s]\n",
      "  7%|▋         | 11.2M/170M [00:00<00:06, 24.2MB/s]\n",
      "  8%|▊         | 14.1M/170M [00:00<00:06, 25.7MB/s]\n",
      " 10%|█         | 17.7M/170M [00:00<00:05, 28.4MB/s]\n",
      " 12%|█▏        | 20.8M/170M [00:00<00:05, 29.4MB/s]\n",
      " 14%|█▍        | 23.8M/170M [00:00<00:05, 29.8MB/s]\n",
      " 16%|█▌        | 26.6M/170M [00:01<00:05, 26.6MB/s]\n",
      " 18%|█▊        | 30.8M/170M [00:01<00:04, 30.2MB/s]\n",
      " 21%|██        | 35.7M/170M [00:01<00:04, 34.4MB/s]\n",
      " 24%|██▎       | 40.0M/170M [00:01<00:03, 36.8MB/s]\n",
      " 26%|██▌       | 43.8M/170M [00:01<00:03, 35.0MB/s]\n",
      " 28%|██▊       | 48.3M/170M [00:01<00:03, 37.9MB/s]\n",
      " 32%|███▏      | 53.8M/170M [00:01<00:02, 42.2MB/s]\n",
      " 34%|███▍      | 58.5M/170M [00:01<00:02, 44.1MB/s]\n",
      " 37%|███▋      | 62.9M/170M [00:01<00:02, 41.1MB/s]\n",
      " 39%|███▉      | 67.1M/170M [00:01<00:02, 41.6MB/s]\n",
      " 42%|████▏     | 71.4M/170M [00:02<00:02, 42.5MB/s]\n",
      " 45%|████▍     | 76.4M/170M [00:02<00:02, 45.0MB/s]\n",
      " 48%|████▊     | 80.8M/170M [00:02<00:02, 42.6MB/s]\n",
      " 50%|█████     | 85.5M/170M [00:02<00:02, 43.1MB/s]\n",
      " 53%|█████▎    | 89.9M/170M [00:02<00:01, 43.8MB/s]\n",
      " 56%|█████▌    | 94.4M/170M [00:02<00:01, 44.9MB/s]\n",
      " 59%|█████▉    | 99.8M/170M [00:02<00:01, 47.8MB/s]\n",
      " 61%|██████▏   | 104M/170M [00:02<00:01, 46.0MB/s] \n",
      " 64%|██████▍   | 109M/170M [00:02<00:01, 38.7MB/s]\n",
      " 66%|██████▋   | 113M/170M [00:03<00:01, 31.5MB/s]\n",
      " 68%|██████▊   | 116M/170M [00:03<00:01, 32.4MB/s]\n",
      " 71%|███████▏  | 121M/170M [00:03<00:01, 36.5MB/s]\n",
      " 74%|███████▎  | 125M/170M [00:03<00:01, 31.0MB/s]\n",
      " 77%|███████▋  | 131M/170M [00:03<00:01, 36.2MB/s]\n",
      " 80%|████████  | 136M/170M [00:03<00:00, 40.3MB/s]\n",
      " 83%|████████▎ | 141M/170M [00:03<00:00, 43.2MB/s]\n",
      " 86%|████████▌ | 146M/170M [00:03<00:00, 41.9MB/s]\n",
      " 88%|████████▊ | 150M/170M [00:04<00:00, 34.4MB/s]\n",
      " 91%|█████████ | 154M/170M [00:04<00:00, 35.3MB/s]\n",
      " 94%|█████████▎| 159M/170M [00:04<00:00, 38.6MB/s]\n",
      " 97%|█████████▋| 165M/170M [00:04<00:00, 43.3MB/s]\n",
      "100%|█████████▉| 169M/170M [00:04<00:00, 41.1MB/s]\n",
      "100%|██████████| 170M/170M [00:04<00:00, 38.4MB/s]\n",
      "Epoch: [0]  [ 0/60]  eta: 0:17:52  lr: 0.000090  loss: 3.5827 (3.5827)  loss_classifier: 0.7385 (0.7385)  loss_box_reg: 0.1523 (0.1523)  loss_mask: 2.6620 (2.6620)  loss_objectness: 0.0224 (0.0224)  loss_rpn_box_reg: 0.0076 (0.0076)  time: 17.8755  data: 15.7209  max mem: 2303\n",
      "Epoch: [0]  [10/60]  eta: 0:03:34  lr: 0.000936  loss: 1.5614 (2.1212)  loss_classifier: 0.4488 (0.4977)  loss_box_reg: 0.1826 (0.1906)  loss_mask: 0.9259 (1.4016)  loss_objectness: 0.0224 (0.0208)  loss_rpn_box_reg: 0.0090 (0.0105)  time: 4.2993  data: 2.6867  max mem: 2862\n",
      "Epoch: [0]  [20/60]  eta: 0:02:41  lr: 0.001783  loss: 0.8708 (1.4310)  loss_classifier: 0.2354 (0.3408)  loss_box_reg: 0.1579 (0.1730)  loss_mask: 0.4009 (0.8836)  loss_objectness: 0.0191 (0.0216)  loss_rpn_box_reg: 0.0099 (0.0120)  time: 3.3333  data: 1.7875  max mem: 2862\n",
      "Epoch: [0]  [30/60]  eta: 0:01:51  lr: 0.002629  loss: 0.5339 (1.1207)  loss_classifier: 0.0958 (0.2568)  loss_box_reg: 0.1155 (0.1595)  loss_mask: 0.2489 (0.6751)  loss_objectness: 0.0105 (0.0176)  loss_rpn_box_reg: 0.0099 (0.0117)  time: 3.3704  data: 1.7992  max mem: 3597\n",
      "Epoch: [0]  [40/60]  eta: 0:01:14  lr: 0.003476  loss: 0.4081 (0.9494)  loss_classifier: 0.0686 (0.2098)  loss_box_reg: 0.1074 (0.1520)  loss_mask: 0.2130 (0.5609)  loss_objectness: 0.0038 (0.0143)  loss_rpn_box_reg: 0.0118 (0.0124)  time: 3.4051  data: 1.7832  max mem: 3597\n",
      "Epoch: [0]  [50/60]  eta: 0:00:35  lr: 0.004323  loss: 0.3306 (0.8261)  loss_classifier: 0.0472 (0.1790)  loss_box_reg: 0.0863 (0.1390)  loss_mask: 0.1740 (0.4831)  loss_objectness: 0.0035 (0.0122)  loss_rpn_box_reg: 0.0118 (0.0128)  time: 3.4263  data: 1.8304  max mem: 3597\n",
      "Epoch: [0]  [59/60]  eta: 0:00:03  lr: 0.005000  loss: 0.2550 (0.7355)  loss_classifier: 0.0365 (0.1566)  loss_box_reg: 0.0558 (0.1238)  loss_mask: 0.1432 (0.4323)  loss_objectness: 0.0019 (0.0106)  loss_rpn_box_reg: 0.0109 (0.0122)  time: 3.3761  data: 1.7830  max mem: 3597\n",
      "Epoch: [0] Total time: 0:03:31 (3.5288 s / it)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Retrying (Retry(total=2, connect=2, read=3, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f14f8db85c0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /history/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/aifxdemo/providers/Microsoft.MachineLearningServices/workspaces/gopalv-ws/experiments/pytorch-peds/runs/pytorch-peds_1583273344_396fd37b/details\n",
      "WARNING - Retrying (Retry(total=1, connect=1, read=3, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f14f8db8278>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /history/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/aifxdemo/providers/Microsoft.MachineLearningServices/workspaces/gopalv-ws/experiments/pytorch-peds/runs/pytorch-peds_1583273344_396fd37b/details\n",
      "WARNING - Retrying (Retry(total=0, connect=0, read=3, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f14f8db8208>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /history/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/aifxdemo/providers/Microsoft.MachineLearningServices/workspaces/gopalv-ws/experiments/pytorch-peds/runs/pytorch-peds_1583273344_396fd37b/details\n"
     ]
    },
    {
     "ename": "ClientRequestError",
     "evalue": "Error occurred in request., ConnectionError: HTTPSConnectionPool(host='westus2.experiments.azureml.net', port=443): Max retries exceeded with url: /history/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/aifxdemo/providers/Microsoft.MachineLearningServices/workspaces/gopalv-ws/experiments/pytorch-peds/runs/pytorch-peds_1583273344_396fd37b/details (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f14bcb32cf8>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 160\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    602\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 169\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.VerifiedHTTPSConnection object at 0x7f14bcb32cf8>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    669\u001b[0m                                 \u001b[0mrelease_conn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelease_conn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                                 **response_kw)\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    669\u001b[0m                                 \u001b[0mrelease_conn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelease_conn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                                 **response_kw)\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    669\u001b[0m                                 \u001b[0mrelease_conn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrelease_conn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m                                 **response_kw)\n\u001b[0m\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    640\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 641\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    642\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='westus2.experiments.azureml.net', port=443): Max retries exceeded with url: /history/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/aifxdemo/providers/Microsoft.MachineLearningServices/workspaces/gopalv-ws/experiments/pytorch-peds/runs/pytorch-peds_1583273344_396fd37b/details (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f14bcb32cf8>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/msrest/universal_http/requests.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 **kwargs)\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='westus2.experiments.azureml.net', port=443): Max retries exceeded with url: /history/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/aifxdemo/providers/Microsoft.MachineLearningServices/workspaces/gopalv-ws/experiments/pytorch-peds/runs/pytorch-peds_1583273344_396fd37b/details (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f14bcb32cf8>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mClientRequestError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-cc0ab15b5cf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mRunDetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/azureml/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0mfile_handle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m                     \u001b[0mwait_post_processing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_post_processing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m                     raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m    576\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/azureml/core/run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[0;34m(self, file_handle, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# TODO use FileWatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;31m# Check whether there is a higher priority log than the one we are currently streaming (current_log)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/azureml/core/run.py\u001b[0m in \u001b[0;36mget_details\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \"\"\"\n\u001b[0;32m--> 928\u001b[0;31m         \u001b[0mdetails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_runstatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_transformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcamel_case_transformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m         \u001b[0minput_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inputDatasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_datasets\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/azureml/_run_impl/run_history_facade.py\u001b[0m in \u001b[0;36mget_runstatus\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_runstatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mrun_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_runstatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_cached_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_status\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/azureml/_restclient/run_client.py\u001b[0m in \u001b[0;36mget_runstatus\u001b[0;34m(self, caller, custom_headers, is_async)\u001b[0m\n\u001b[1;32m     96\u001b[0m             is_async=is_async, caller=caller, custom_headers=custom_headers)\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_with_run_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_details\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     def create_child_run(self, child_run_id, script_name=None,\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/azureml/_restclient/run_client.py\u001b[0m in \u001b[0;36m_execute_with_run_arguments\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_with_run_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_with_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_arguments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_combine_with_run_paginated_dto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_to_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/azureml/_restclient/workspace_client.py\u001b[0m in \u001b[0;36m_execute_with_arguments\u001b[0;34m(self, func, args_list, *args, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_paginated_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mErrorResponseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mServiceException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/azureml/_restclient/clientbase.py\u001b[0m in \u001b[0;36m_call_api\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mAsyncTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ident\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_parent_logger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_with_base_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_paginated_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/azureml/_restclient/clientbase.py\u001b[0m in \u001b[0;36m_execute_with_base_arguments\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mtotal_retry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretries\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         return ClientBase._execute_func_internal(\n\u001b[0;32m--> 274\u001b[0;31m             back_off, total_retry, self._logger, func, _noop_reset, *args, **kwargs)\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/azureml/_restclient/clientbase.py\u001b[0m in \u001b[0;36m_execute_func_internal\u001b[0;34m(cls, back_off, total_retry, logger, func, reset_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                 \u001b[0mleft_retry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mback_off\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_retry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_retry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mreset_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# reset_func is expected to undo any side effects from a failed func call.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/azureml/_restclient/clientbase.py\u001b[0m in \u001b[0;36m_handle_retry\u001b[0;34m(cls, back_off, left_retry, total_retry, error, logger, func)\u001b[0m\n\u001b[1;32m    338\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRETRY_EXCEPTIONS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback_off\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_retry\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mleft_retry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/azureml/_restclient/clientbase.py\u001b[0m in \u001b[0;36m_execute_func_internal\u001b[0;34m(cls, back_off, total_retry, logger, func, reset_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mleft_retry\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0mleft_retry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mback_off\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_retry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_retry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/azureml/_restclient/operations/run_operations.py\u001b[0m in \u001b[0;36mget_details\u001b[0;34m(self, subscription_id, resource_group_name, workspace_name, experiment_name, run_id, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;31m# Construct and send request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moperation_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/msrest/service_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, headers, content, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stream'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0mpipeline_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0;31m# There is too much thing that expects this method to return a \"requests.Response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;31m# to break it in a compatible release.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/msrest/pipeline/__init__.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mpipeline_request\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: Request[HTTPRequestType]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mfirst_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl_policies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_impl_policies\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfirst_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPSender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAbstractContextManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mHTTPRequestType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHTTPResponseType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/msrest/pipeline/__init__.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/msrest/pipeline/requests.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             except (oauth2.rfc6749.errors.InvalidGrantError,\n\u001b[1;32m     74\u001b[0m                     oauth2.rfc6749.errors.TokenExpiredError) as err:\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/msrest/pipeline/requests.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mold_max_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/msrest/pipeline/__init__.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/msrest/pipeline/requests.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m         return Response(\n\u001b[1;32m    192\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/msrest/universal_http/requests.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \"\"\"\n\u001b[1;32m    332\u001b[0m         \u001b[0mrequests_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_configure_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRequestsHTTPSender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrequests_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/msrest/universal_http/requests.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Error occurred in request.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mraise_with_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mClientRequestError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRequestsClientResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/msrest/exceptions.py\u001b[0m in \u001b[0;36mraise_with_traceback\u001b[0;34m(exception, message, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/msrest/universal_http/requests.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 **kwargs)\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Error occurred in request.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/azureml/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientRequestError\u001b[0m: Error occurred in request., ConnectionError: HTTPSConnectionPool(host='westus2.experiments.azureml.net', port=443): Max retries exceeded with url: /history/v1.0/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourceGroups/aifxdemo/providers/Microsoft.MachineLearningServices/workspaces/gopalv-ws/experiments/pytorch-peds/runs/pytorch-peds_1583273344_396fd37b/details (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f14bcb32cf8>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',))"
     ]
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Run\n",
    "\n",
    "run = Run(run_id='pytorch-peds_1583262435_ac3ea423', experiment=experiment)\n",
    "model = run.register_model(model_name='pytorch_peds', model_path='outputs/model.pt')\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "path = model.download(target_dir='.', exist_ok=True)\n",
    "path\n",
    "\n",
    "# model = torch.load(path)\n",
    "#torch.load(model.get_model_path(model_name='outputs/model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from azureml.core import Dataset\n",
    "from data import PennFudanDataset\n",
    "from script import get_transform\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = torch.load('./model.pt', map_location=device)\n",
    "\n",
    "penn_ds = Dataset.get_by_name(workspace=ws, name='penn_ds')\n",
    "dataset_test = PennFudanDataset(penn_ds, get_transform(train=False))\n",
    "\n",
    "\n",
    "\n",
    "# pick one image from the test set\n",
    "img, _ = dataset_test[0]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in list_runs:\n",
    "    print(run.get_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.estimator import Estimator\n",
    "\n",
    "script_params = {\n",
    "    '--num_epochs': 10,\n",
    "    '--output_dir': './outputs'\n",
    "}\n",
    "\n",
    "estimator = Estimator(source_directory=project_folder, \n",
    "                      script_params=script_params,\n",
    "                      compute_target=compute_target,\n",
    "                      entry_script='script.py',\n",
    "                      use_gpu=True,\n",
    "                      conda_dependencies_file='conda-dependencies.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(estimator)\n",
    "print(run.get_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transforms=None):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        with self.dataset.mount() as mount_context:\n",
    "            self.imgs = list(sorted(os.listdir(os.path.join(mount_context.mount_point, \"PNGImages\"))))\n",
    "            self.masks = list(sorted(os.listdir(os.path.join(mount_context.mount_point, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        with self.dataset.mount() as mount_context:\n",
    "            img_path = os.path.join(mount_context.mount_point, \"PNGImages\", self.imgs[idx])\n",
    "            mask_path = os.path.join(mount_context.mount_point, \"PedMasks\", self.masks[idx])\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            # note that we haven't converted the mask to RGB,\n",
    "            # because each color corresponds to a different instance\n",
    "            # with 0 being background\n",
    "            mask = Image.open(mask_path)\n",
    "\n",
    "            mask = np.array(mask)\n",
    "            # instances are encoded as different colors\n",
    "            obj_ids = np.unique(mask)\n",
    "            # first id is the background, so remove it\n",
    "            obj_ids = obj_ids[1:]\n",
    "\n",
    "            # split the color-encoded mask into a set\n",
    "            # of binary masks\n",
    "            masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "            # get bounding box coordinates for each mask\n",
    "            num_objs = len(obj_ids)\n",
    "            boxes = []\n",
    "            for i in range(num_objs):\n",
    "                pos = np.where(masks[i])\n",
    "                xmin = np.min(pos[1])\n",
    "                xmax = np.max(pos[1])\n",
    "                ymin = np.min(pos[0])\n",
    "                ymax = np.max(pos[0])\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            # there is only one class\n",
    "            labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "            image_id = torch.tensor([idx])\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            # suppose all instances are not crowd\n",
    "            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = labels\n",
    "            target[\"masks\"] = masks\n",
    "            target[\"image_id\"] = image_id\n",
    "            target[\"area\"] = area\n",
    "            target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "            if self.transforms is not None:\n",
    "                img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "help(\"modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# from engine import train_one_epoch, evaluate\n",
    "# import utils\n",
    "# import transforms as T\n",
    "\n",
    "# def get_transform(train):\n",
    "#     transforms = []\n",
    "#     # converts the image, a PIL image, into a PyTorch Tensor\n",
    "#     transforms.append(T.ToTensor())\n",
    "#     if train:\n",
    "#         # during training, randomly flip the training images\n",
    "#         # and ground-truth for data augmentation\n",
    "#         transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "#     return T.Compose(transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# # use our dataset and defined transformations\n",
    "# dataset = PennFudanDataset(penn_ds, get_transform(train=True))\n",
    "# dataset_test = PennFudanDataset(penn_ds, get_transform(train=False))\n",
    "\n",
    "# # split the dataset in train and test set\n",
    "# torch.manual_seed(1)\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# # define training and validation data loaders\n",
    "# data_loader = torch.utils.data.DataLoader(\n",
    "#     dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "#     collate_fn=utils.collate_fn)\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "#     collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # our dataset has two classes only - background and person\n",
    "# num_classes = 2\n",
    "\n",
    "# # get the model using our helper function\n",
    "# model = get_instance_segmentation_model(num_classes)\n",
    "# # move model to the right device\n",
    "# model.to(device)\n",
    "\n",
    "# # construct an optimizer\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "#                             momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# # and a learning rate scheduler which decreases the learning rate by\n",
    "# # 10x every 3 epochs\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "#                                                step_size=3,\n",
    "#                                                gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # train for one epoch, printing every 10 iterations\n",
    "#     train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "#     # update the learning rate\n",
    "#     lr_scheduler.step()\n",
    "#     # evaluate on the test dataset\n",
    "#     evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pick one image from the test set\n",
    "# img, _ = dataset_test[0]\n",
    "# # put the model in evaluation mode\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "azureml",
   "language": "python",
   "name": "azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
