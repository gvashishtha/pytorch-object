{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './pytorch-peds'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./pytorch-peds/script.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('script.py', project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'cocoapi' already exists and is not an empty directory.\n",
      "PyTorch-object-detection-1.ipynb \u001b[1m\u001b[36mcocoapi\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mbuild\u001b[m\u001b[m                            config.json\n",
      "/Users/gkv/Code/PT-Azure-maskrcnn/cocoapi/PythonAPI\n",
      "Makefile             \u001b[1m\u001b[36mdist\u001b[m\u001b[m                 setup.py\n",
      "\u001b[1m\u001b[36mPennFudanPed\u001b[m\u001b[m         engine.py            test.zip\n",
      "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m          pycocoDemo.ipynb     transforms.py\n",
      "\u001b[1m\u001b[36mbuild\u001b[m\u001b[m                pycocoEvalDemo.ipynb utils.py\n",
      "coco_eval.py         \u001b[1m\u001b[36mpycocotools\u001b[m\u001b[m          \u001b[1m\u001b[36mvision\u001b[m\u001b[m\n",
      "coco_utils.py        \u001b[1m\u001b[36mpycocotools.egg-info\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mcocoapi\u001b[m\u001b[m              script.py\n",
      "Requirement already satisfied: cython in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (0.29.14)\n",
      "Requirement already satisfied: numpy in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (1.18.1)\n",
      "running build_ext\n",
      "skipping 'pycocotools/_mask.c' Cython extension (up-to-date)\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing pycocotools.egg-info/PKG-INFO\n",
      "writing dependency_links to pycocotools.egg-info/dependency_links.txt\n",
      "writing requirements to pycocotools.egg-info/requires.txt\n",
      "writing top-level names to pycocotools.egg-info/top_level.txt\n",
      "reading manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
      "writing manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.macosx-10.9-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.macosx-10.9-x86_64/egg\n",
      "creating build/bdist.macosx-10.9-x86_64/egg/pycocotools\n",
      "copying build/lib.macosx-10.9-x86_64-3.8/pycocotools/coco.py -> build/bdist.macosx-10.9-x86_64/egg/pycocotools\n",
      "copying build/lib.macosx-10.9-x86_64-3.8/pycocotools/mask.py -> build/bdist.macosx-10.9-x86_64/egg/pycocotools\n",
      "copying build/lib.macosx-10.9-x86_64-3.8/pycocotools/__init__.py -> build/bdist.macosx-10.9-x86_64/egg/pycocotools\n",
      "copying build/lib.macosx-10.9-x86_64-3.8/pycocotools/cocoeval.py -> build/bdist.macosx-10.9-x86_64/egg/pycocotools\n",
      "copying build/lib.macosx-10.9-x86_64-3.8/pycocotools/_mask.cpython-38-darwin.so -> build/bdist.macosx-10.9-x86_64/egg/pycocotools\n",
      "byte-compiling build/bdist.macosx-10.9-x86_64/egg/pycocotools/coco.py to coco.cpython-38.pyc\n",
      "byte-compiling build/bdist.macosx-10.9-x86_64/egg/pycocotools/mask.py to mask.cpython-38.pyc\n",
      "byte-compiling build/bdist.macosx-10.9-x86_64/egg/pycocotools/__init__.py to __init__.cpython-38.pyc\n",
      "byte-compiling build/bdist.macosx-10.9-x86_64/egg/pycocotools/cocoeval.py to cocoeval.cpython-38.pyc\n",
      "creating stub loader for pycocotools/_mask.cpython-38-darwin.so\n",
      "byte-compiling build/bdist.macosx-10.9-x86_64/egg/pycocotools/_mask.py to _mask.cpython-38.pyc\n",
      "creating build/bdist.macosx-10.9-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/PKG-INFO -> build/bdist.macosx-10.9-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/SOURCES.txt -> build/bdist.macosx-10.9-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/dependency_links.txt -> build/bdist.macosx-10.9-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/requires.txt -> build/bdist.macosx-10.9-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/top_level.txt -> build/bdist.macosx-10.9-x86_64/egg/EGG-INFO\n",
      "writing build/bdist.macosx-10.9-x86_64/egg/EGG-INFO/native_libs.txt\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "pycocotools.__pycache__._mask.cpython-38: module references __file__\n",
      "creating 'dist/pycocotools-2.0-py3.8-macosx-10.9-x86_64.egg' and adding 'build/bdist.macosx-10.9-x86_64/egg' to it\n",
      "removing 'build/bdist.macosx-10.9-x86_64/egg' (and everything under it)\n",
      "Processing pycocotools-2.0-py3.8-macosx-10.9-x86_64.egg\n",
      "removing '/Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/pycocotools-2.0-py3.8-macosx-10.9-x86_64.egg' (and everything under it)\n",
      "creating /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/pycocotools-2.0-py3.8-macosx-10.9-x86_64.egg\n",
      "Extracting pycocotools-2.0-py3.8-macosx-10.9-x86_64.egg to /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages\n",
      "pycocotools 2.0 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/pycocotools-2.0-py3.8-macosx-10.9-x86_64.egg\n",
      "Processing dependencies for pycocotools==2.0\n",
      "Searching for matplotlib==3.2.0rc3\n",
      "Best match: matplotlib 3.2.0rc3\n",
      "Processing matplotlib-3.2.0rc3-py3.8-macosx-10.9-x86_64.egg\n",
      "matplotlib 3.2.0rc3 is already the active version in easy-install.pth\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/matplotlib-3.2.0rc3-py3.8-macosx-10.9-x86_64.egg\n",
      "Searching for Cython==0.29.14\n",
      "Best match: Cython 0.29.14\n",
      "Adding Cython 0.29.14 to easy-install.pth file\n",
      "Installing cygdb script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "Installing cython script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "Installing cythonize script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages\n",
      "Searching for setuptools==45.1.0.post20200127\n",
      "Best match: setuptools 45.1.0.post20200127\n",
      "Adding setuptools 45.1.0.post20200127 to easy-install.pth file\n",
      "Installing easy_install script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages\n",
      "Searching for python-dateutil==2.8.1\n",
      "Best match: python-dateutil 2.8.1\n",
      "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages\n",
      "Searching for pyparsing==2.4.6\n",
      "Best match: pyparsing 2.4.6\n",
      "Processing pyparsing-2.4.6-py3.8.egg\n",
      "pyparsing 2.4.6 is already the active version in easy-install.pth\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/pyparsing-2.4.6-py3.8.egg\n",
      "Searching for numpy==1.18.1\n",
      "Best match: numpy 1.18.1\n",
      "Adding numpy 1.18.1 to easy-install.pth file\n",
      "Installing f2py script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "Installing f2py3 script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "Installing f2py3.8 script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages\n",
      "Searching for kiwisolver==1.1.0\n",
      "Best match: kiwisolver 1.1.0\n",
      "Processing kiwisolver-1.1.0-py3.8-macosx-10.9-x86_64.egg\n",
      "kiwisolver 1.1.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/kiwisolver-1.1.0-py3.8-macosx-10.9-x86_64.egg\n",
      "Searching for cycler==0.10.0\n",
      "Best match: cycler 0.10.0\n",
      "Processing cycler-0.10.0-py3.8.egg\n",
      "cycler 0.10.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/cycler-0.10.0-py3.8.egg\n",
      "Searching for six==1.14.0\n",
      "Best match: six 1.14.0\n",
      "Adding six 1.14.0 to easy-install.pth file\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages\n",
      "Finished processing dependencies for pycocotools==2.0\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#sys.path.insert(0,'./')\n",
    "# Install pycocotools\n",
    "# !git clone https://github.com/cocodataset/cocoapi.git\n",
    "# !ls\n",
    "# %cd cocoapi/PythonAPI\n",
    "# !ls\n",
    "# !pip install cython\n",
    "# !pip install numpy\n",
    "# !python setup.py build_ext install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.85\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning diagnostics collection on. \n"
     ]
    }
   ],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: gopalv-ws\n",
      "Azure region: westus2\n",
      "Subscription id: 15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\n",
      "Resource group: aifxdemo\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2020-02-16T23:53:03.831000+00:00', 'errors': None, 'creationTime': '2020-02-05T18:19:06.976503+00:00', 'modifiedTime': '2020-02-05T18:20:28.120151+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 2, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'LowPriority', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./test.zip', <http.client.HTTPMessage at 0x10cdfdaf0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    " \n",
    "url = 'https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip'\n",
    " \n",
    "urllib.request.urlretrieve(url, './test.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "zip = ZipFile(file='./test.zip')\n",
    "zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mAnnotation\u001b[m\u001b[m            \u001b[1m\u001b[36mPedMasks\u001b[m\u001b[m              readme.txt\r\n",
      "\u001b[1m\u001b[36mPNGImages\u001b[m\u001b[m             added-object-list.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls PennFudanPed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspaceblobstore AzureBlob gopalvws3790775563 azureml-blobstore-e47496c6-9688-4277-a05b-ceb722514b9d\n"
     ]
    }
   ],
   "source": [
    "# get the default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": true,
    "outputHidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 512 files\n",
      "Target already exists. Skipping upload for data/added-object-list.txt\n",
      "Target already exists. Skipping upload for data/readme.txt\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00048_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00049_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00072_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00073_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00001_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00093_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00092_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00076_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00077_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00005_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00004_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00031_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00030_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00042_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00043_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00035_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00034_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00046_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00047_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00084_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00085_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00016_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00017_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00065_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00064_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00012_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00013_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00028_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00029_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00061_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00060_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00055_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00054_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00026_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00027_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00051_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00050_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00022_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00023_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00018_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00019_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00011_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00010_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00083_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00082_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00058_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00059_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00062_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00063_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00015_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00014_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00066_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00067_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00068_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00069_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00052_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00053_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00021_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00020_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00089_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00088_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00056_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00057_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00025_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00024_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00074_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00094_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00095_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00006_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00007_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00071_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00070_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00002_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00003_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00038_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00039_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00036_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00037_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00045_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00044_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00032_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00033_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00008_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00009_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00041_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00040_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00056_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00057_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00025_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00024_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00068_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00069_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00052_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00053_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00021_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00020_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00087_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00086_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00015_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00014_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00066_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00067_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00011_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00010_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00058_mask.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00059_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00062_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00063_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00032_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00033_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00008_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00009_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00041_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00040_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00036_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00037_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00045_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00044_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00071_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00070_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00002_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00003_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00090_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00091_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00038_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00039_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00075_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00074_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00006_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00007_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00035_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00034_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00046_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00047_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00031_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00030_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00078_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00079_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00042_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00043_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00096_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00005_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00004_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00048_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00049_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00072_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00073_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00001_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00051_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00050_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00022_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00023_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00018_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00019_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00055_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00054_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00026_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00027_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00012_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00013_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00028_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00029_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00080_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00081_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00061_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00060_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00016_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00017_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00065_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00064_mask.png\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00013.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00007.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00014.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00028.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00029.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00015.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00001.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00006.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00012.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00004.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00010.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00038.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00017.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00003.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00002.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00016.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00039.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00011.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00005.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00029.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00001.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00015.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00012.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00006.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00007.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00013.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00014.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00028.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00016.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00002.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00039.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00005.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00011.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00010.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00004.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00038.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00003.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00017.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00070.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00064.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00058.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for data/Annotation/FudanPed00063.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00062.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00059.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00065.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00071.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00067.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00073.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00074.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00060.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00048.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00049.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00061.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00072.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00066.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00089.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00062.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00076.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00059.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00071.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00065.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00064.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00070.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00058.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00077.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00063.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00088.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00049.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00075.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00061.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00066.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00072.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00073.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00067.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00060.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00074.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00048.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00092.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00086.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00051.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00045.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00079.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00042.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00056.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00057.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00043.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00078.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00044.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00050.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00087.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00093.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00085.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00091.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00046.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00052.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00055.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00041.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00069.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00068.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00040.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00054.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00053.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00047.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00090.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00084.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00080.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00094.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00043.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00057.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00050.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00044.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00045.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00051.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00056.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00042.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00095.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00081.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00083.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00068.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00054.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00040.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00047.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00053.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00052.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00046.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00041.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00055.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00069.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00082.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00096.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00032.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00026.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00021.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00035.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00009.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00008.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00034.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00020.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00027.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00033.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00025.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00031.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00019.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00036.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00022.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00023.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00037.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00018.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00030.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00024.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for data/Annotation/PennPed00008.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00020.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00034.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00033.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00027.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00026.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00032.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00035.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00021.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00009.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00037.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00023.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00018.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00024.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00030.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00031.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00025.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00019.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00022.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00036.txt\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00062.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00059.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00071.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00065.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00064.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00070.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00058.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00063.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00049.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00061.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00066.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00072.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00073.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00067.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00060.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00074.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00048.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00070.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00064.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00058.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00063.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00077.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00088.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00089.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00076.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00062.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00059.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00065.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00071.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00067.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00073.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00074.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00060.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00048.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00049.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00061.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00075.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00072.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00066.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00029.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00001.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00015.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00012.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00006.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00007.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00013.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00014.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00028.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00016.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00002.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00039.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00005.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00011.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00010.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00004.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00038.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00003.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00017.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00013.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00007.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00014.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00028.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00029.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00015.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00001.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00006.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00012.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00004.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00010.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00038.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00017.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00003.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00002.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00016.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00039.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00011.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00005.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00008.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00020.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00034.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00033.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00027.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00026.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00032.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00035.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00021.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00009.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00037.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00023.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00018.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for data/PNGImages/PennPed00024.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00030.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00031.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00025.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00019.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00022.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00036.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00032.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00026.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00021.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00035.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00009.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00008.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00034.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00020.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00027.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00033.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00025.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00031.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00019.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00036.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00022.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00023.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00037.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00018.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00030.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00024.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00043.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00057.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00078.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00050.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00044.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00093.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00087.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00086.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00092.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00045.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00051.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00079.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00056.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00042.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00068.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00054.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00040.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00047.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00053.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00084.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00090.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00091.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00085.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00052.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00046.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00041.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00055.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00069.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00051.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00045.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00042.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00056.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00081.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00095.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00094.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00080.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00057.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00043.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00044.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00050.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00046.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00052.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00055.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00041.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00069.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00096.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00082.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00083.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00068.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00040.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00054.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00053.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00047.png\n",
      "Uploaded 0 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_26d2302af8f64b43a3c0bc9a249bffab"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.upload('./PennFudanPed', target_path='data', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'data')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"1cd2dd50-ae6d-44f9-81e2-f0044c862efe\",\n",
       "    \"name\": \"penn_ds\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"Penn Fudan pedestrian data\",\n",
       "    \"workspace\": \"Workspace.create(name='gopalv-ws', subscription_id='15ae9cb6-95c1-483d-a0e3-b1a1a3b06324', resource_group='aifxdemo')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "# create a FileDataset pointing to files in 'animals' folder and its subfolders recursively\n",
    "datastore_paths = [(ds, 'data')]\n",
    "penn_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "penn_ds.register(workspace=ws,\n",
    "                 name='penn_ds',\n",
    "                 description='Penn Fudan pedestrian data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputHidden": true
   },
   "outputs": [],
   "source": [
    "# dataset_name = 'penn_ds'\n",
    "\n",
    "# # Get a dataset by name\n",
    "# penn1_ds = Dataset.get_by_name(workspace=ws, name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azureml.data.dataset_consumption_config.DatasetConsumptionConfig at 0x122723130>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "# import glob\n",
    "\n",
    "\n",
    "# with penn_ds.mount() as mount_context:\n",
    "#     # list top level mounted files and folders in the dataset\n",
    "#     imgs_path =(os.path.join(mount_context.mount_point, 'PNGImages'))\n",
    "#     print(f'imgs_path is {imgs_path}')\n",
    "#     print(list(sorted(os.listdir(imgs_path))))\n",
    "#     Image.open(os.path.join(mount_context.mount_point, 'PNGImages/PennPed00036.png'))\n",
    "penn1_ds.as_named_input('test').as_mount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PennFudanDataset at 0x122a2f040>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PennFudanDataset(penn_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE     README.rst  hubconf.py  setup.cfg   \u001b[1m\u001b[36mtest\u001b[m\u001b[m        tox.ini\n",
      "MANIFEST.in \u001b[1m\u001b[36mdocs\u001b[m\u001b[m        \u001b[1m\u001b[36mreferences\u001b[m\u001b[m  setup.py    \u001b[1m\u001b[36mtorchvision\u001b[m\u001b[m\n",
      "/Users/gkv/MS/pytorch-object\n",
      "\u001b[1m\u001b[36mPennFudanPed\u001b[m\u001b[m                     \u001b[1m\u001b[36mpytorch-peds\u001b[m\u001b[m\n",
      "PyTorch-object-detection-1.ipynb script.py\n",
      "coco_eval.py                     test.zip\n",
      "coco_utils.py                    transforms.py\n",
      "config.json                      utils.py\n",
      "engine.py                        \u001b[1m\u001b[36mvision\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "%cd ..\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vision' already exists and is not an empty directory.\n",
      "error: pathspec 'v0.3.0' did not match any file(s) known to git\n",
      "/Users/gkv/MS/pytorch-object/vision\n",
      "/Users/gkv/MS/pytorch-object\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pytorch/vision.git\n",
    "\n",
    "!git checkout v0.3.0\n",
    "\n",
    "%cd vision\n",
    "!cp references/detection/utils.py ../\n",
    "!cp references/detection/transforms.py ../\n",
    "!cp references/detection/coco_eval.py ../\n",
    "!cp references/detection/engine.py ../\n",
    "!cp references/detection/coco_utils.py ../\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "os.listdir()\n",
    "\n",
    "files_to_copy = ['utils', 'transforms', 'coco_eval', 'engine', 'coco_utils']\n",
    "for file in files_to_copy:\n",
    "    shutil.copy('./vision/references/detection/'+ file + '.py', project_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch-peds'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "my_env = Environment(name='maskr-docker')\n",
    "my_env.docker.enabled = True\n",
    "with open(\"Dockerfile\", \"r\") as f:\n",
    "    dockerfile_contents_of_your_base_image=f.read()\n",
    "my_env.docker.base_dockerfile=dockerfile_contents_of_your_base_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: pytorch-peds,\n",
      "Id: pytorch-peds_1582496081_2fd369c5,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Starting)\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "# follow pattern from here: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments#use-environments-for-training\n",
    "\n",
    "# Add training script to run config\n",
    "runconfig = ScriptRunConfig(source_directory=\".\", script=\"script.py\")\n",
    "\n",
    "# Attach compute target to run config\n",
    "runconfig.run_config.target = \"local\"\n",
    "\n",
    "my_env.docker.base_image = None\n",
    "# Attach environment to run config\n",
    "runconfig.run_config.environment = my_env\n",
    "\n",
    "# Submit run \n",
    "run = experiment.submit(runconfig)\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'runId': 'pytorch-peds_1582496081_2fd369c5', 'target': 'local', 'status': 'Starting', 'properties': {'_azureml.ComputeTargetType': 'local', 'ContentSnapshotId': 'be542fea-e9cc-4a28-8f2f-c93815655bb3', 'azureml.git.repository_uri': 'https://github.com/gvashishtha/pytorch-object.git', 'mlflow.source.git.repoURL': 'https://github.com/gvashishtha/pytorch-object.git', 'azureml.git.branch': 'docker', 'mlflow.source.git.branch': 'docker', 'azureml.git.commit': '618dbb834d141b9c9dda355c158c52ea77e45dc6', 'mlflow.source.git.commit': '618dbb834d141b9c9dda355c158c52ea77e45dc6', 'azureml.git.dirty': 'False'}, 'inputDatasets': [], 'runDefinition': {'script': 'script.py', 'useAbsolutePath': False, 'arguments': [], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'local', 'dataReferences': {}, 'data': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'environment': {'name': 'maskr-docker', 'version': 'Autosave_2020-02-23T22:11:06Z_168aa0dc', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}], 'name': 'azureml_1b417bb747e35859ebf611fb43071e9c'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': None, 'baseDockerfile': '# Copyright (c) Microsoft Corporation. All rights reserved.\\n# Licensed under the MIT License.\\n\\nFROM mcr.microsoft.com/azureml/o16n-base/python-assets@sha256:20a8b655a3e5b9b0db8ddf70d03d048a7cf49e569c4f0382198b1ee77631a6ad AS inferencing-assets\\n\\n# Tag: cuda:10.1-cudnn7-devel-ubuntu18.04\\n# Env: CUDA_VERSION=10.1.243\\n# Env: CUDA_PKG_VERSION=10-1=10.1.243-1\\n# Env: NCCL_VERSION=2.4.8\\n# Env: CUDNN_VERSION=7.6.3.30\\n# Env: NVIDIA_VISIBLE_DEVICES=all\\n# Env: NVIDIA_DRIVER_CAPABILITIES=compute,utility\\n# Env: NVIDIA_REQUIRE_CUDA=cuda>=10.1 brand=tesla,driver>=384,driver<385 brand=tesla,driver>=410,driver<411\\n# Label: com.nvidia.cuda.version=10.1.243\\n# Label: com.nvidia.cudnn.version=7.6.3.30\\n# Label: com.nvidia.volumes.needed=nvidia_driver\\n# Ubuntu 18.04\\nFROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04\\n\\nUSER root:root\\n\\nENV com.nvidia.cuda.version $CUDA_VERSION\\nENV com.nvidia.volumes.needed nvidia_driver\\nENV LANG=C.UTF-8 LC_ALL=C.UTF-8\\nENV DEBIAN_FRONTEND noninteractive\\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\\nENV NCCL_DEBUG=INFO\\nENV HOROVOD_GPU_ALLREDUCE=NCCL\\n\\n# Install Common Dependencies\\nRUN apt-get update && \\\\\\n    apt-get install -y --no-install-recommends \\\\\\n    # SSH and RDMA\\n    libmlx4-1 \\\\\\n    libmlx5-1 \\\\\\n    librdmacm1 \\\\\\n    libibverbs1 \\\\\\n    libmthca1 \\\\\\n    libdapl2 \\\\\\n    dapl2-utils \\\\\\n    openssh-client \\\\\\n    openssh-server \\\\\\n    iproute2 && \\\\\\n    # Others\\n    apt-get install -y \\\\\\n    build-essential \\\\\\n    bzip2=1.0.6-8.1ubuntu0.2 \\\\\\n    libbz2-1.0=1.0.6-8.1ubuntu0.2 \\\\\\n    systemd \\\\\\n    git=1:2.17.1-1ubuntu0.4 \\\\\\n    wget \\\\\\n    cpio \\\\\\n    libsm6 \\\\\\n    libxext6 \\\\\\n    libxrender-dev \\\\\\n    fuse && \\\\\\n    apt-get clean -y && \\\\\\n    rm -rf /var/lib/apt/lists/*\\n\\n# Inference\\n# Copy logging utilities, nginx and rsyslog configuration files, IOT server binary, etc.\\nCOPY --from=inferencing-assets /artifacts /var/\\nRUN /var/requirements/install_system_requirements.sh && \\\\\\n    cp /var/configuration/rsyslog.conf /etc/rsyslog.conf && \\\\\\n    cp /var/configuration/nginx.conf /etc/nginx/sites-available/app && \\\\\\n    ln -s /etc/nginx/sites-available/app /etc/nginx/sites-enabled/app && \\\\\\n    rm -f /etc/nginx/sites-enabled/default\\nENV SVDIR=/var/runit\\nENV WORKER_TIMEOUT=300\\nEXPOSE 5001 8883 8888\\n\\n# Conda Environment\\nENV MINICONDA_VERSION 4.5.11\\nENV PATH /opt/miniconda/bin:$PATH\\nRUN wget -qO /tmp/miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-${MINICONDA_VERSION}-Linux-x86_64.sh && \\\\\\n    bash /tmp/miniconda.sh -bf -p /opt/miniconda && \\\\\\n    conda clean -ay && \\\\\\n    rm -rf /opt/miniconda/pkgs && \\\\\\n    rm /tmp/miniconda.sh && \\\\\\n    find / -type d -name __pycache__ | xargs rm -rf\\n\\n# Open-MPI installation\\nENV OPENMPI_VERSION 3.1.2\\nRUN mkdir /tmp/openmpi && \\\\\\n    cd /tmp/openmpi && \\\\\\n    wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-${OPENMPI_VERSION}.tar.gz && \\\\\\n    tar zxf openmpi-${OPENMPI_VERSION}.tar.gz && \\\\\\n    cd openmpi-${OPENMPI_VERSION} && \\\\\\n    ./configure --enable-orterun-prefix-by-default && \\\\\\n    make -j $(nproc) all && \\\\\\n    make install && \\\\\\n    ldconfig && \\\\\\n    rm -rf /tmp/openmpi\\n', 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}}, 'logFiles': {'azureml-logs/60_control_log.txt': 'https://gopalvws3790775563.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-peds_1582496081_2fd369c5/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=nNngHKqwpsL6WsW9CbOxZIBypThsSOPbMQoosnojzf4%3D&st=2020-02-23T22%3A04%3A44Z&se=2020-02-24T06%3A14%3A44Z&sp=r'}}\n"
     ]
    }
   ],
   "source": [
    "# to get more details of your run\n",
    "print(run.get_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ceed2e18d343e7b84b9924b69272b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Preparing\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/pytorch-peds/runs/pytorch-peds_1582496081_2fd369c5?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/aifxdemo/workspaces/gopalv-ws\", \"run_id\": \"pytorch-peds_1582496081_2fd369c5\", \"run_properties\": {\"run_id\": \"pytorch-peds_1582496081_2fd369c5\", \"created_utc\": \"2020-02-23T22:14:42.72151Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"local\", \"ContentSnapshotId\": \"be542fea-e9cc-4a28-8f2f-c93815655bb3\", \"azureml.git.repository_uri\": \"https://github.com/gvashishtha/pytorch-object.git\", \"mlflow.source.git.repoURL\": \"https://github.com/gvashishtha/pytorch-object.git\", \"azureml.git.branch\": \"docker\", \"mlflow.source.git.branch\": \"docker\", \"azureml.git.commit\": \"618dbb834d141b9c9dda355c158c52ea77e45dc6\", \"mlflow.source.git.commit\": \"618dbb834d141b9c9dda355c158c52ea77e45dc6\", \"azureml.git.dirty\": \"False\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": null, \"status\": \"Preparing\", \"log_files\": {\"azureml-logs/60_control_log.txt\": \"https://gopalvws3790775563.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-peds_1582496081_2fd369c5/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=Y56tsa%2B%2Bmsz7YDL9zdR1I6Uwqfa%2FpaDRDSmdP0PdzT8%3D&st=2020-02-23T22%3A09%3A49Z&se=2020-02-24T06%3A19%3A49Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/60_control_log.txt\"]], \"run_duration\": \"0:05:06\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"Streaming log file azureml-logs/60_control_log.txt\\nStarting the daemon thread to refresh tokens in background for process with pid = 38906\\nRunning: ['/bin/bash', '/private/var/folders/zq/sdr9gtc92pd15gf53xb1tct80000gn/T/azureml_runs/pytorch-peds_1582496081_2fd369c5/azureml-environment-setup/docker_env_checker.sh']\\n\\nMaterialized image not found on target: azureml/azureml_314fdf099d7c5d83ce53da44e576a098\\n\\n\\nLogging experiment preparation status in history service.\\nRunning: ['/bin/bash', '/private/var/folders/zq/sdr9gtc92pd15gf53xb1tct80000gn/T/azureml_runs/pytorch-peds_1582496081_2fd369c5/azureml-environment-setup/docker_env_builder.sh']\\nRunning: ['docker', 'build', '-f', 'azureml-environment-setup/Dockerfile', '-t', 'azureml/azureml_314fdf099d7c5d83ce53da44e576a098', '.']\\nSending build context to Docker daemon  677.9kB\\nStep 1/35 : FROM mcr.microsoft.com/azureml/o16n-base/python-assets@sha256:20a8b655a3e5b9b0db8ddf70d03d048a7cf49e569c4f0382198b1ee77631a6ad AS inferencing-assets\\nsha256:20a8b655a3e5b9b0db8ddf70d03d048a7cf49e569c4f0382198b1ee77631a6ad: Pulling from azureml/o16n-base/python-assets\\nc3c7f3e444b9: Pulling fs layer\\nc3c7f3e444b9: Verifying Checksum\\nc3c7f3e444b9: Download complete\\nc3c7f3e444b9: Pull complete\\nDigest: sha256:20a8b655a3e5b9b0db8ddf70d03d048a7cf49e569c4f0382198b1ee77631a6ad\\nStatus: Downloaded newer image for mcr.microsoft.com/azureml/o16n-base/python-assets@sha256:20a8b655a3e5b9b0db8ddf70d03d048a7cf49e569c4f0382198b1ee77631a6ad\\n ---> 1fab9272a4c4\\nStep 2/35 : FROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04\\n10.1-cudnn7-devel-ubuntu18.04: Pulling from nvidia/cuda\\n7ddbc47eeb70: Pulling fs layer\\nc1bbdc448b72: Pulling fs layer\\n8c3b70e39044: Pulling fs layer\\n45d437916d57: Pulling fs layer\\nd8f1569ddae6: Pulling fs layer\\n85386706b020: Pulling fs layer\\nee9b457b77d0: Pulling fs layer\\nbe4f3343ecd3: Pulling fs layer\\n30b4effda4fd: Pulling fs layer\\nb398e882f414: Pulling fs layer\\nbe4f3343ecd3: Waiting\\n30b4effda4fd: Waiting\\nb398e882f414: Waiting\\nd8f1569ddae6: Waiting\\n85386706b020: Waiting\\nc1bbdc448b72: Verifying Checksum\\nc1bbdc448b72: Download complete\\n8c3b70e39044: Verifying Checksum\\n8c3b70e39044: Download complete\\n45d437916d57: Verifying Checksum\\n45d437916d57: Download complete\\n7ddbc47eeb70: Verifying Checksum\\n7ddbc47eeb70: Download complete\\nd8f1569ddae6: Download complete\\nee9b457b77d0: Verifying Checksum\\nee9b457b77d0: Download complete\\n7ddbc47eeb70: Pull complete\\nc1bbdc448b72: Pull complete\\n85386706b020: Download complete\\n8c3b70e39044: Pull complete\\n45d437916d57: Pull complete\\nd8f1569ddae6: Pull complete\\n85386706b020: Pull complete\\nee9b457b77d0: Pull complete\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": true, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.85\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: pytorch-peds_1582496081_2fd369c5\n",
      "Web View: https://ml.azure.com/experiments/pytorch-peds/runs/pytorch-peds_1582496081_2fd369c5?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/aifxdemo/workspaces/gopalv-ws\n",
      "\n",
      "Streaming azureml-logs/60_control_log.txt\n",
      "=========================================\n",
      "\n",
      "Streaming log file azureml-logs/60_control_log.txt\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 38906\n",
      "Running: ['/bin/bash', '/private/var/folders/zq/sdr9gtc92pd15gf53xb1tct80000gn/T/azureml_runs/pytorch-peds_1582496081_2fd369c5/azureml-environment-setup/docker_env_checker.sh']\n",
      "\n",
      "Materialized image not found on target: azureml/azureml_314fdf099d7c5d83ce53da44e576a098\n",
      "\n",
      "\n",
      "Logging experiment preparation status in history service.\n",
      "Running: ['/bin/bash', '/private/var/folders/zq/sdr9gtc92pd15gf53xb1tct80000gn/T/azureml_runs/pytorch-peds_1582496081_2fd369c5/azureml-environment-setup/docker_env_builder.sh']\n",
      "Running: ['docker', 'build', '-f', 'azureml-environment-setup/Dockerfile', '-t', 'azureml/azureml_314fdf099d7c5d83ce53da44e576a098', '.']\n",
      "Sending build context to Docker daemon  677.9kB\n",
      "Step 1/35 : FROM mcr.microsoft.com/azureml/o16n-base/python-assets@sha256:20a8b655a3e5b9b0db8ddf70d03d048a7cf49e569c4f0382198b1ee77631a6ad AS inferencing-assets\n",
      "sha256:20a8b655a3e5b9b0db8ddf70d03d048a7cf49e569c4f0382198b1ee77631a6ad: Pulling from azureml/o16n-base/python-assets\n",
      "c3c7f3e444b9: Pulling fs layer\n",
      "c3c7f3e444b9: Verifying Checksum\n",
      "c3c7f3e444b9: Download complete\n",
      "c3c7f3e444b9: Pull complete\n",
      "Digest: sha256:20a8b655a3e5b9b0db8ddf70d03d048a7cf49e569c4f0382198b1ee77631a6ad\n",
      "Status: Downloaded newer image for mcr.microsoft.com/azureml/o16n-base/python-assets@sha256:20a8b655a3e5b9b0db8ddf70d03d048a7cf49e569c4f0382198b1ee77631a6ad\n",
      " ---> 1fab9272a4c4\n",
      "Step 2/35 : FROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04\n",
      "10.1-cudnn7-devel-ubuntu18.04: Pulling from nvidia/cuda\n",
      "7ddbc47eeb70: Pulling fs layer\n",
      "c1bbdc448b72: Pulling fs layer\n",
      "8c3b70e39044: Pulling fs layer\n",
      "45d437916d57: Pulling fs layer\n",
      "d8f1569ddae6: Pulling fs layer\n",
      "85386706b020: Pulling fs layer\n",
      "ee9b457b77d0: Pulling fs layer\n",
      "be4f3343ecd3: Pulling fs layer\n",
      "30b4effda4fd: Pulling fs layer\n",
      "b398e882f414: Pulling fs layer\n",
      "be4f3343ecd3: Waiting\n",
      "30b4effda4fd: Waiting\n",
      "b398e882f414: Waiting\n",
      "d8f1569ddae6: Waiting\n",
      "85386706b020: Waiting\n",
      "c1bbdc448b72: Verifying Checksum\n",
      "c1bbdc448b72: Download complete\n",
      "8c3b70e39044: Verifying Checksum\n",
      "8c3b70e39044: Download complete\n",
      "45d437916d57: Verifying Checksum\n",
      "45d437916d57: Download complete\n",
      "7ddbc47eeb70: Verifying Checksum\n",
      "7ddbc47eeb70: Download complete\n",
      "d8f1569ddae6: Download complete\n",
      "ee9b457b77d0: Verifying Checksum\n",
      "ee9b457b77d0: Download complete\n",
      "7ddbc47eeb70: Pull complete\n",
      "c1bbdc448b72: Pull complete\n",
      "85386706b020: Download complete\n",
      "8c3b70e39044: Pull complete\n",
      "45d437916d57: Pull complete\n",
      "d8f1569ddae6: Pull complete\n",
      "85386706b020: Pull complete\n",
      "ee9b457b77d0: Pull complete\n",
      "b398e882f414: Verifying Checksum\n",
      "b398e882f414: Download complete\n",
      "be4f3343ecd3: Verifying Checksum\n",
      "be4f3343ecd3: Download complete\n",
      "be4f3343ecd3: Pull complete\n",
      "30b4effda4fd: Verifying Checksum\n",
      "30b4effda4fd: Download complete\n",
      "30b4effda4fd: Pull complete\n",
      "b398e882f414: Pull complete\n",
      "Digest: sha256:557de4ba2cb674029ffb602bed8f748d44d59bb7db9daa746ea72a102406d3ec\n",
      "Status: Downloaded newer image for nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04\n",
      " ---> b4879c167fc1\n",
      "Step 3/35 : USER root:root\n",
      " ---> Running in ad09fce9f8b0\n",
      "Removing intermediate container ad09fce9f8b0\n",
      " ---> e7fa6ceb440d\n",
      "Step 4/35 : ENV com.nvidia.cuda.version $CUDA_VERSION\n",
      " ---> Running in 59a39e1ad215\n",
      "Removing intermediate container 59a39e1ad215\n",
      " ---> f45b8aeb4eff\n",
      "Step 5/35 : ENV com.nvidia.volumes.needed nvidia_driver\n",
      " ---> Running in 9814abf0ea26\n",
      "Removing intermediate container 9814abf0ea26\n",
      " ---> ed4773446f6e\n",
      "Step 6/35 : ENV LANG=C.UTF-8 LC_ALL=C.UTF-8\n",
      " ---> Running in 41ca2154b1bf\n",
      "Removing intermediate container 41ca2154b1bf\n",
      " ---> 99f111c33cf9\n",
      "Step 7/35 : ENV DEBIAN_FRONTEND noninteractive\n",
      " ---> Running in e43871893470\n",
      "Removing intermediate container e43871893470\n",
      " ---> 74c01961f673\n",
      "Step 8/35 : ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\n",
      " ---> Running in 7cbaa6270cb5\n",
      "Removing intermediate container 7cbaa6270cb5\n",
      " ---> d979d13d76f1\n",
      "Step 9/35 : ENV NCCL_DEBUG=INFO\n",
      " ---> Running in de15c13e9f9c\n",
      "Removing intermediate container de15c13e9f9c\n",
      " ---> 9d0555506b0a\n",
      "Step 10/35 : ENV HOROVOD_GPU_ALLREDUCE=NCCL\n",
      " ---> Running in dbca2aa04add\n",
      "Removing intermediate container dbca2aa04add\n",
      " ---> 6b071900bc56\n",
      "Step 11/35 : RUN apt-get update &&     apt-get install -y --no-install-recommends     libmlx4-1     libmlx5-1     librdmacm1     libibverbs1     libmthca1     libdapl2     dapl2-utils     openssh-client     openssh-server     iproute2 &&     apt-get install -y     build-essential     bzip2=1.0.6-8.1ubuntu0.2     libbz2-1.0=1.0.6-8.1ubuntu0.2     systemd     git=1:2.17.1-1ubuntu0.4     wget     cpio     libsm6     libxext6     libxrender-dev     fuse &&     apt-get clean -y &&     rm -rf /var/lib/apt/lists/*\n",
      " ---> Running in 26d9bee8b47e\n",
      "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [564 B]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]\n",
      "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
      "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
      "Get:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
      "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [132 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [31.0 kB]\n",
      "Get:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [30.4 kB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [7348 B]\n",
      "Get:13 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [835 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [823 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1350 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [44.7 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1127 kB]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [11.4 kB]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [2496 B]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4252 B]\n",
      "Fetched 17.8 MB in 15s (1201 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  libbsd0 libedit2 libelf1 libgssapi-krb5-2 libk5crypto3 libkeyutils1\n",
      "  libkrb5-3 libkrb5support0 libmnl0 libnl-3-200 libnl-route-3-200 libssl1.0.0\n",
      "  libwrap0 openssh-sftp-server ucf\n",
      "Suggested packages:\n",
      "  iproute2-doc krb5-doc krb5-user keychain libpam-ssh monkeysphere ssh-askpass\n",
      "  molly-guard rssh ufw\n",
      "Recommended packages:\n",
      "  libatm1 libxtables12 krb5-locales xauth libpam-systemd ncurses-term\n",
      "  ssh-import-id\n",
      "The following NEW packages will be installed:\n",
      "  dapl2-utils ibverbs-providers iproute2 libbsd0 libdapl2 libedit2 libelf1\n",
      "  libgssapi-krb5-2 libibverbs1 libk5crypto3 libkeyutils1 libkrb5-3\n",
      "  libkrb5support0 libmnl0 libnl-3-200 libnl-route-3-200 librdmacm1 libssl1.0.0\n",
      "  libwrap0 openssh-client openssh-server openssh-sftp-server ucf\n",
      "0 upgraded, 23 newly installed, 0 to remove and 28 not upgraded.\n",
      "Need to get 4299 kB of archives.\n",
      "After this operation, 16.3 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libelf1 amd64 0.170-0.4ubuntu0.1 [44.8 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmnl0 amd64 1.0.4-2 [12.3 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 iproute2 amd64 4.15.0-2ubuntu1 [721 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbsd0 amd64 0.8.7-1ubuntu0.1 [41.6 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 ucf all 3.0038 [50.5 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 libedit2 amd64 3.1-20170329-1 [76.9 kB]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libkrb5support0 amd64 1.16-2ubuntu0.1 [30.9 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libk5crypto3 amd64 1.16-2ubuntu0.1 [85.6 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 libkeyutils1 amd64 1.5.9-9.2ubuntu2 [8720 B]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libkrb5-3 amd64 1.16-2ubuntu0.1 [279 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgssapi-krb5-2 amd64 1.16-2ubuntu0.1 [122 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libssl1.0.0 amd64 1.0.2n-1ubuntu5.3 [1088 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssh-client amd64 1:7.6p1-4ubuntu0.3 [614 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnl-3-200 amd64 3.2.29-0ubuntu3 [52.8 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnl-route-3-200 amd64 3.2.29-0ubuntu3 [146 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libibverbs1 amd64 17.1-1ubuntu0.2 [44.4 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ibverbs-providers amd64 17.1-1ubuntu0.2 [160 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 libwrap0 amd64 7.6.q-27 [46.3 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssh-sftp-server amd64 1:7.6p1-4ubuntu0.3 [45.6 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssh-server amd64 1:7.6p1-4ubuntu0.3 [333 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 librdmacm1 amd64 17.1-1ubuntu0.2 [56.1 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libdapl2 amd64 2.1.10.1.f1e05b7a-3 [121 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu bionic/universe amd64 dapl2-utils amd64 2.1.10.1.f1e05b7a-3 [119 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 4299 kB in 5s (847 kB/s)\n",
      "Selecting previously unselected package libelf1:amd64.\n",
      "(Reading database ... 12067 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libelf1_0.170-0.4ubuntu0.1_amd64.deb ...\n",
      "Unpacking libelf1:amd64 (0.170-0.4ubuntu0.1) ...\n",
      "Selecting previously unselected package libmnl0:amd64.\n",
      "Preparing to unpack .../01-libmnl0_1.0.4-2_amd64.deb ...\n",
      "Unpacking libmnl0:amd64 (1.0.4-2) ...\n",
      "Selecting previously unselected package iproute2.\n",
      "Preparing to unpack .../02-iproute2_4.15.0-2ubuntu1_amd64.deb ...\n",
      "Unpacking iproute2 (4.15.0-2ubuntu1) ...\n",
      "Selecting previously unselected package libbsd0:amd64.\n",
      "Preparing to unpack .../03-libbsd0_0.8.7-1ubuntu0.1_amd64.deb ...\n",
      "Unpacking libbsd0:amd64 (0.8.7-1ubuntu0.1) ...\n",
      "Selecting previously unselected package ucf.\n",
      "Preparing to unpack .../04-ucf_3.0038_all.deb ...\n",
      "Moving old data out of the way\n",
      "Unpacking ucf (3.0038) ...\n",
      "Selecting previously unselected package libedit2:amd64.\n",
      "Preparing to unpack .../05-libedit2_3.1-20170329-1_amd64.deb ...\n",
      "Unpacking libedit2:amd64 (3.1-20170329-1) ...\n",
      "Selecting previously unselected package libkrb5support0:amd64.\n",
      "Preparing to unpack .../06-libkrb5support0_1.16-2ubuntu0.1_amd64.deb ...\n",
      "Unpacking libkrb5support0:amd64 (1.16-2ubuntu0.1) ...\n",
      "Selecting previously unselected package libk5crypto3:amd64.\n",
      "Preparing to unpack .../07-libk5crypto3_1.16-2ubuntu0.1_amd64.deb ...\n",
      "Unpacking libk5crypto3:amd64 (1.16-2ubuntu0.1) ...\n",
      "Selecting previously unselected package libkeyutils1:amd64.\n",
      "Preparing to unpack .../08-libkeyutils1_1.5.9-9.2ubuntu2_amd64.deb ...\n",
      "Unpacking libkeyutils1:amd64 (1.5.9-9.2ubuntu2) ...\n",
      "Selecting previously unselected package libkrb5-3:amd64.\n",
      "Preparing to unpack .../09-libkrb5-3_1.16-2ubuntu0.1_amd64.deb ...\n",
      "Unpacking libkrb5-3:amd64 (1.16-2ubuntu0.1) ...\n",
      "Selecting previously unselected package libgssapi-krb5-2:amd64.\n",
      "Preparing to unpack .../10-libgssapi-krb5-2_1.16-2ubuntu0.1_amd64.deb ...\n",
      "Unpacking libgssapi-krb5-2:amd64 (1.16-2ubuntu0.1) ...\n",
      "Selecting previously unselected package libssl1.0.0:amd64.\n",
      "Preparing to unpack .../11-libssl1.0.0_1.0.2n-1ubuntu5.3_amd64.deb ...\n",
      "Unpacking libssl1.0.0:amd64 (1.0.2n-1ubuntu5.3) ...\n",
      "Selecting previously unselected package openssh-client.\n",
      "Preparing to unpack .../12-openssh-client_1%3a7.6p1-4ubuntu0.3_amd64.deb ...\n",
      "Unpacking openssh-client (1:7.6p1-4ubuntu0.3) ...\n",
      "Selecting previously unselected package libnl-3-200:amd64.\n",
      "Preparing to unpack .../13-libnl-3-200_3.2.29-0ubuntu3_amd64.deb ...\n",
      "Unpacking libnl-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
      "Selecting previously unselected package libnl-route-3-200:amd64.\n",
      "Preparing to unpack .../14-libnl-route-3-200_3.2.29-0ubuntu3_amd64.deb ...\n",
      "Unpacking libnl-route-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
      "Selecting previously unselected package libibverbs1:amd64.\n",
      "Preparing to unpack .../15-libibverbs1_17.1-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking libibverbs1:amd64 (17.1-1ubuntu0.2) ...\n",
      "Selecting previously unselected package ibverbs-providers:amd64.\n",
      "Preparing to unpack .../16-ibverbs-providers_17.1-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking ibverbs-providers:amd64 (17.1-1ubuntu0.2) ...\n",
      "Selecting previously unselected package libwrap0:amd64.\n",
      "Preparing to unpack .../17-libwrap0_7.6.q-27_amd64.deb ...\n",
      "Unpacking libwrap0:amd64 (7.6.q-27) ...\n",
      "Selecting previously unselected package openssh-sftp-server.\n",
      "Preparing to unpack .../18-openssh-sftp-server_1%3a7.6p1-4ubuntu0.3_amd64.deb ...\n",
      "Unpacking openssh-sftp-server (1:7.6p1-4ubuntu0.3) ...\n",
      "Selecting previously unselected package openssh-server.\n",
      "Preparing to unpack .../19-openssh-server_1%3a7.6p1-4ubuntu0.3_amd64.deb ...\n",
      "Unpacking openssh-server (1:7.6p1-4ubuntu0.3) ...\n",
      "Selecting previously unselected package librdmacm1:amd64.\n",
      "Preparing to unpack .../20-librdmacm1_17.1-1ubuntu0.2_amd64.deb ...\n",
      "Unpacking librdmacm1:amd64 (17.1-1ubuntu0.2) ...\n",
      "Selecting previously unselected package libdapl2.\n",
      "Preparing to unpack .../21-libdapl2_2.1.10.1.f1e05b7a-3_amd64.deb ...\n",
      "Unpacking libdapl2 (2.1.10.1.f1e05b7a-3) ...\n",
      "Selecting previously unselected package dapl2-utils.\n",
      "Preparing to unpack .../22-dapl2-utils_2.1.10.1.f1e05b7a-3_amd64.deb ...\n",
      "Unpacking dapl2-utils (2.1.10.1.f1e05b7a-3) ...\n",
      "Setting up libedit2:amd64 (3.1-20170329-1) ...\n",
      "Setting up libssl1.0.0:amd64 (1.0.2n-1ubuntu5.3) ...\n",
      "Setting up libelf1:amd64 (0.170-0.4ubuntu0.1) ...\n",
      "Setting up libbsd0:amd64 (0.8.7-1ubuntu0.1) ...\n",
      "Setting up libkrb5support0:amd64 (1.16-2ubuntu0.1) ...\n",
      "Setting up ucf (3.0038) ...\n",
      "Setting up libkeyutils1:amd64 (1.5.9-9.2ubuntu2) ...\n",
      "Setting up libnl-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
      "Setting up libmnl0:amd64 (1.0.4-2) ...\n",
      "Setting up libwrap0:amd64 (7.6.q-27) ...\n",
      "Setting up libk5crypto3:amd64 (1.16-2ubuntu0.1) ...\n",
      "Setting up libnl-route-3-200:amd64 (3.2.29-0ubuntu3) ...\n",
      "Setting up iproute2 (4.15.0-2ubuntu1) ...\n",
      "Setting up libkrb5-3:amd64 (1.16-2ubuntu0.1) ...\n",
      "Setting up libibverbs1:amd64 (17.1-1ubuntu0.2) ...\n",
      "Setting up librdmacm1:amd64 (17.1-1ubuntu0.2) ...\n",
      "Setting up libgssapi-krb5-2:amd64 (1.16-2ubuntu0.1) ...\n",
      "Setting up libdapl2 (2.1.10.1.f1e05b7a-3) ...\n",
      "Setting up ibverbs-providers:amd64 (17.1-1ubuntu0.2) ...\n",
      "Setting up openssh-client (1:7.6p1-4ubuntu0.3) ...\n",
      "Setting up dapl2-utils (2.1.10.1.f1e05b7a-3) ...\n",
      "Setting up openssh-sftp-server (1:7.6p1-4ubuntu0.3) ...\n",
      "Setting up openssh-server (1:7.6p1-4ubuntu0.3) ...\n",
      "\n",
      "Creating config file /etc/ssh/sshd_config with new version\n",
      "Creating SSH2 RSA key; this may take some time ...\n",
      "2048 SHA256:TVW59SU7lhhN8TWHKGA9+Hf/gRnjoHINVgzJPt9e9Tg root@26d9bee8b47e (RSA)\n",
      "Creating SSH2 ECDSA key; this may take some time ...\n",
      "256 SHA256:rRDUmshx1V/E+TA6Va8wt9Ai6xBJMCzFg2eux0Pf0Ho root@26d9bee8b47e (ECDSA)\n",
      "Creating SSH2 ED25519 key; this may take some time ...\n",
      "256 SHA256:ffOxPmxpep7wiuvY7YzyQ9P1cYrn+SG3QMgk61nrCKA root@26d9bee8b47e (ED25519)\n",
      "invoke-rc.d: could not determine current runlevel\n",
      "invoke-rc.d: policy-rc.d denied execution of start.\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "\u001b[91mE: Version '1:2.17.1-1ubuntu0.4' for 'git' was not found\n",
      "The command '/bin/sh -c apt-get update &&     apt-get install -y --no-install-recommends     libmlx4-1     libmlx5-1     librdmacm1     libibverbs1     libmthca1     libdapl2     dapl2-utils     openssh-client     openssh-server     iproute2 &&     apt-get install -y     build-essential     bzip2=1.0.6-8.1ubuntu0.2     libbz2-1.0=1.0.6-8.1ubuntu0.2     systemd     git=1:2.17.1-1ubuntu0.4     wget     cpio     libsm6     libxext6     libxrender-dev     fuse &&     apt-get clean -y &&     rm -rf /var/lib/apt/lists/*' returned a non-zero code: 100\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "CalledProcessError(100, ['docker', 'build', '-f', 'azureml-environment-setup/Dockerfile', '-t', 'azureml/azureml_314fdf099d7c5d83ce53da44e576a098', '.'])\n",
      "\n",
      "Building docker image failed with exit code: 100\n",
      "\n",
      "\n",
      "\n",
      "Logging error in history service: Failed to run ['/bin/bash', '/private/var/folders/zq/sdr9gtc92pd15gf53xb1tct80000gn/T/azureml_runs/pytorch-peds_1582496081_2fd369c5/azureml-environment-setup/docker_env_builder.sh'] \n",
      " Exit code 1 \n",
      "Details can be found in azureml-logs/60_control_log.txt log file.\n",
      "\n",
      "Uploading control log...\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: pytorch-peds_1582496081_2fd369c5\n",
      "Web View: https://ml.azure.com/experiments/pytorch-peds/runs/pytorch-peds_1582496081_2fd369c5?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/aifxdemo/workspaces/gopalv-ws\n",
      "\n",
      "Warnings:\n",
      "{\n",
      "  \"error\": {\n",
      "    \"code\": \"ServiceError\",\n",
      "    \"message\": \"Failed to run ['/bin/bash', '/private/var/folders/zq/sdr9gtc92pd15gf53xb1tct80000gn/T/azureml_runs/pytorch-peds_1582496081_2fd369c5/azureml-environment-setup/docker_env_builder.sh'] \\n Exit code 1 \\nDetails can be found in azureml-logs/60_control_log.txt log file.\",\n",
      "    \"detailsUri\": null,\n",
      "    \"target\": null,\n",
      "    \"details\": [],\n",
      "    \"innerError\": null,\n",
      "    \"debugInfo\": null\n",
      "  },\n",
      "  \"correlation\": null,\n",
      "  \"environment\": null,\n",
      "  \"location\": null,\n",
      "  \"time\": \"0001-01-01T00:00:00+00:00\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "ActivityFailedException",
     "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Building docker image failed with exit code: 100\",\n        \"details\": []\n    },\n    \"time\": \"2020-02-23T22:29:46.207038Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Building docker image failed with exit code: 100\\\",\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"2020-02-23T22:29:46.207038Z\\\"\\n}\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-cc0ab15b5cf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mRunDetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.8/site-packages/azureml/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m                 self._stream_run_output(\n\u001b[0m\u001b[1;32m    655\u001b[0m                     \u001b[0mfile_handle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0mwait_post_processing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_post_processing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.8/site-packages/azureml/core/run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[0;34m(self, file_handle, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mActivityFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Building docker image failed with exit code: 100\",\n        \"details\": []\n    },\n    \"time\": \"2020-02-23T22:29:46.207038Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Building docker image failed with exit code: 100\\\",\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"2020-02-23T22:29:46.207038Z\\\"\\n}\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - framework_version is not specified, defaulting to version 1.3.\n",
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['torchvision']. We cannot guarantee image build will succeed.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "script_params = {\n",
    "    '--num_epochs': 10,\n",
    "    '--output_dir': './outputs'\n",
    "}\n",
    "\n",
    "estimator = PyTorch(source_directory=project_folder, \n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script='script.py',\n",
    "                    use_gpu=True,\n",
    "                    pip_packages=['torchvision>=0.5.0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transforms=None):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        with self.dataset.mount() as mount_context:\n",
    "            self.imgs = list(sorted(os.listdir(os.path.join(mount_context.mount_point, \"PNGImages\"))))\n",
    "            self.masks = list(sorted(os.listdir(os.path.join(mount_context.mount_point, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        with self.dataset.mount() as mount_context:\n",
    "            img_path = os.path.join(mount_context.mount_point, \"PNGImages\", self.imgs[idx])\n",
    "            mask_path = os.path.join(mount_context.mount_point, \"PedMasks\", self.masks[idx])\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            # note that we haven't converted the mask to RGB,\n",
    "            # because each color corresponds to a different instance\n",
    "            # with 0 being background\n",
    "            mask = Image.open(mask_path)\n",
    "\n",
    "            mask = np.array(mask)\n",
    "            # instances are encoded as different colors\n",
    "            obj_ids = np.unique(mask)\n",
    "            # first id is the background, so remove it\n",
    "            obj_ids = obj_ids[1:]\n",
    "\n",
    "            # split the color-encoded mask into a set\n",
    "            # of binary masks\n",
    "            masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "            # get bounding box coordinates for each mask\n",
    "            num_objs = len(obj_ids)\n",
    "            boxes = []\n",
    "            for i in range(num_objs):\n",
    "                pos = np.where(masks[i])\n",
    "                xmin = np.min(pos[1])\n",
    "                xmax = np.max(pos[1])\n",
    "                ymin = np.min(pos[0])\n",
    "                ymax = np.max(pos[0])\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            # there is only one class\n",
    "            labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "            image_id = torch.tensor([idx])\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            # suppose all instances are not crowd\n",
    "            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = labels\n",
    "            target[\"masks\"] = masks\n",
    "            target[\"image_id\"] = image_id\n",
    "            target[\"area\"] = area\n",
    "            target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "            if self.transforms is not None:\n",
    "                img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torchvision in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (from torchvision) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (from torchvision) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied, skipping upgrade: torch==1.4.0 in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (from torchvision) (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n",
      "\n",
      "Please wait a moment while I gather a list of all available modules...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/IPython/kernel/__init__.py:12: ShimWarning: The `IPython.kernel` package has been deprecated since IPython 4.0.You should import from ipykernel or jupyter_client instead.\n",
      "  warn(\"The `IPython.kernel` package has been deprecated since IPython 4.0.\"\n",
      "/Users/gkv/miniconda3/envs/azureml/lib/python3.8/pkgutil.py:107: VisibleDeprecationWarning: zmq.eventloop.minitornado is deprecated in pyzmq 14.0 and will be removed.\n",
      "    Install tornado itself to use zmq with the tornado IOLoop.\n",
      "    \n",
      "  yield from walk_packages(path, info.name+'.', onerror)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cython              asynchat            inspect             random\n",
      "IPython             asyncio             io                  re\n",
      "OpenSSL             asyncore            ipaddress           readline\n",
      "PIL                 atexit              ipykernel           reprlib\n",
      "PyQt5               attr                ipykernel_launcher  requests\n",
      "__future__          audioop             ipython_genutils    requests_oauthlib\n",
      "_abc                automl              ipywidgets          resource\n",
      "_ast                autoreload          isodate             rlcompleter\n",
      "_asyncio            azureml             itertools           rmagic\n",
      "_bisect             backcall            jaraco              runpy\n",
      "_blake2             backports           jedi                sched\n",
      "_bootlocale         base64              jeepney             secrets\n",
      "_bz2                bdb                 jinja2              secretstorage\n",
      "_cffi_backend       binascii            jmespath            select\n",
      "_codecs             binhex              json                selectors\n",
      "_codecs_cn          bisect              jsonpickle          send2trash\n",
      "_codecs_hk          bleach              jsonschema          setup\n",
      "_codecs_iso2022     builtins            jupyter             setuptools\n",
      "_codecs_jp          bz2                 jupyter_client      shelve\n",
      "_codecs_kr          cProfile            jupyter_console     shlex\n",
      "_codecs_tw          caffe2              jupyter_core        shutil\n",
      "_collections        calendar            jwt                 signal\n",
      "_collections_abc    certifi             keyword             sip\n",
      "_compat_pickle      cffi                kiwisolver          sipconfig\n",
      "_compression        cgi                 lib2to3             sipdistutils\n",
      "_contextvars        cgitb               linecache           site\n",
      "_crypt              chardet             locale              six\n",
      "_csv                chunk               logging             smtpd\n",
      "_ctypes             cloudpickle         lzma                smtplib\n",
      "_ctypes_test        cmath               mailbox             sndhdr\n",
      "_curses             cmd                 mailcap             socket\n",
      "_curses_panel       coco_eval           markupsafe          socketserver\n",
      "_datetime           coco_utils          marshal             sqlite3\n",
      "_dbm                code                math                sre_compile\n",
      "_decimal            codecs              matplotlib          sre_constants\n",
      "_dummy_thread       codeop              mccabe              sre_parse\n",
      "_elementtree        collections         mimetypes           ssl\n",
      "_functools          colorsys            mistune             stat\n",
      "_hashlib            compileall          mkl                 statistics\n",
      "_heapq              concurrent          mkl_fft             storemagic\n",
      "_imp                configparser        mmap                string\n",
      "_io                 contextlib          modulefinder        stringprep\n",
      "_json               contextlib2         more_itertools      struct\n",
      "_locale             contextvars         mpl_toolkits        subprocess\n",
      "_lsprof             copy                msrest              sunau\n",
      "_lzma               copyreg             msrestazure         symbol\n",
      "_markupbase         crypt               multiprocessing     sympyprinting\n",
      "_md5                cryptography        nbconvert           symtable\n",
      "_multibytecodec     csv                 nbformat            sys\n",
      "_multiprocessing    ctypes              ndg                 sysconfig\n",
      "_opcode             curses              netrc               syslog\n",
      "_operator           cycler              nis                 tabnanny\n",
      "_osx_support        cython              nntplib             tarfile\n",
      "_pickle             cythonmagic         notebook            telnetlib\n",
      "_posixshmem         dataclasses         ntpath              tempfile\n",
      "_posixsubprocess    datetime            nturl2path          terminado\n",
      "_py_abc             dateutil            numbers             termios\n",
      "_pydecimal          dbm                 numpy               test\n",
      "_pyio               decimal             oauthlib            testpath\n",
      "_pyrsistent_version decorator           opcode              tests\n",
      "_queue              defusedxml          operator            textwrap\n",
      "_random             difflib             optparse            this\n",
      "_ruamel_yaml        dis                 os                  threading\n",
      "_scproxy            distro              pandocfilters       time\n",
      "_sha1               distutils           parser              timeit\n",
      "_sha256             docker              parso               tkinter\n",
      "_sha3               doctest             pathlib             token\n",
      "_sha512             dotnetcore2         pathspec            tokenize\n",
      "_signal             dummy_threading     pdb                 torch\n",
      "_sitebuiltins       easy_install        pexpect             torchvision\n",
      "_socket             email               pickle              tornado\n",
      "_sqlite3            encodings           pickleshare         trace\n",
      "_sre                engine              pickletools         traceback\n",
      "_ssl                ensurepip           pip                 tracemalloc\n",
      "_stat               entrypoints         pipes               traitlets\n",
      "_statistics         enum                pkg_resources       transforms\n",
      "_string             errno               pkgutil             tty\n",
      "_strptime           faulthandler        platform            turtle\n",
      "_struct             fcntl               plistlib            turtledemo\n",
      "_symtable           filecmp             poplib              types\n",
      "_sysconfigdata__darwin_darwin fileinput           posix               typing\n",
      "_sysconfigdata_aarch64_conda_cos7_linux_gnu flake8              posixpath           unicodedata\n",
      "_sysconfigdata_i686_conda_cos6_linux_gnu fnmatch             pprint              unittest\n",
      "_sysconfigdata_powerpc64le_conda_cos7_linux_gnu formatter           profile             urllib\n",
      "_sysconfigdata_x86_64_apple_darwin13_4_0 fractions           prometheus_client   urllib3\n",
      "_sysconfigdata_x86_64_conda_cos6_linux_gnu ftplib              prompt_toolkit      utils\n",
      "_testbuffer         functools           pstats              uu\n",
      "_testcapi           fuse                pty                 uuid\n",
      "_testimportmultiple gc                  ptyprocess          venv\n",
      "_testinternalcapi   genericpath         pvectorc            warnings\n",
      "_testmultiphase     getopt              pwd                 wave\n",
      "_thread             getpass             py_compile          wcwidth\n",
      "_threading_local    gettext             pyasn1              weakref\n",
      "_tkinter            glob                pyclbr              webbrowser\n",
      "_tracemalloc        grp                 pycocotools         webencodings\n",
      "_uuid               gzip                pycodestyle         websocket\n",
      "_warnings           hashlib             pycparser           wheel\n",
      "_weakref            heapq               pydoc               widgetsnbextension\n",
      "_weakrefset         hmac                pydoc_data          wsgiref\n",
      "_xxsubinterpreters  html                pyexpat             xdrlib\n",
      "_xxtestfuzz         http                pyflakes            xml\n",
      "abc                 hubconf             pygments            xmlrpc\n",
      "adal                idlelib             pylab               xxlimited\n",
      "aifc                idna                pyparsing           xxsubtype\n",
      "antigravity         imaplib             pyrsistent          zipapp\n",
      "applicationinsights imghdr              pytz                zipfile\n",
      "appnope             imp                 pyximport           zipimport\n",
      "argparse            importlib           qtconsole           zipp\n",
      "array               importlib_metadata  queue               zlib\n",
      "ast                 inflect             quopri              zmq\n",
      "\n",
      "Enter any module name to get more help.  Or, type \"modules spam\" to search\n",
      "for modules whose name or summary contain the string \"spam\".\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gkv/miniconda3/envs/azureml/lib/python3.8/pkgutil.py:92: MatplotlibDeprecationWarning: \n",
      "The mpl_toolkits.axes_grid module was deprecated in Matplotlib 2.1 and will be removed two minor releases later. Use mpl_toolkits.axes_grid1 and mpl_toolkits.axisartist, which provide the same functionality instead.\n",
      "  __import__(info.name)\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "help(\"modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# from engine import train_one_epoch, evaluate\n",
    "# import utils\n",
    "# import transforms as T\n",
    "\n",
    "# def get_transform(train):\n",
    "#     transforms = []\n",
    "#     # converts the image, a PIL image, into a PyTorch Tensor\n",
    "#     transforms.append(T.ToTensor())\n",
    "#     if train:\n",
    "#         # during training, randomly flip the training images\n",
    "#         # and ground-truth for data augmentation\n",
    "#         transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "#     return T.Compose(transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# # use our dataset and defined transformations\n",
    "# dataset = PennFudanDataset(penn_ds, get_transform(train=True))\n",
    "# dataset_test = PennFudanDataset(penn_ds, get_transform(train=False))\n",
    "\n",
    "# # split the dataset in train and test set\n",
    "# torch.manual_seed(1)\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# # define training and validation data loaders\n",
    "# data_loader = torch.utils.data.DataLoader(\n",
    "#     dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "#     collate_fn=utils.collate_fn)\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "#     collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # our dataset has two classes only - background and person\n",
    "# num_classes = 2\n",
    "\n",
    "# # get the model using our helper function\n",
    "# model = get_instance_segmentation_model(num_classes)\n",
    "# # move model to the right device\n",
    "# model.to(device)\n",
    "\n",
    "# # construct an optimizer\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "#                             momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# # and a learning rate scheduler which decreases the learning rate by\n",
    "# # 10x every 3 epochs\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "#                                                step_size=3,\n",
    "#                                                gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_one_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c750cf7ed566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_one_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # train for one epoch, printing every 10 iterations\n",
    "#     train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "#     # update the learning rate\n",
    "#     lr_scheduler.step()\n",
    "#     # evaluate on the test dataset\n",
    "#     evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pick one image from the test set\n",
    "# img, _ = dataset_test[0]\n",
    "# # put the model in evaluation mode\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "azureml",
   "language": "python",
   "name": "azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
