{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "project_folder = './pytorch-peds'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./pytorch-peds/script.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('script.py', project_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'cocoapi' already exists and is not an empty directory.\n",
      "PyTorch-object-detection-1.ipynb \u001b[1m\u001b[36mcocoapi\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mbuild\u001b[m\u001b[m                            config.json\n",
      "/Users/gkv/Code/PT-Azure-maskrcnn/cocoapi/PythonAPI\n",
      "Makefile             \u001b[1m\u001b[36mdist\u001b[m\u001b[m                 setup.py\n",
      "\u001b[1m\u001b[36mPennFudanPed\u001b[m\u001b[m         engine.py            test.zip\n",
      "\u001b[1m\u001b[36m__pycache__\u001b[m\u001b[m          pycocoDemo.ipynb     transforms.py\n",
      "\u001b[1m\u001b[36mbuild\u001b[m\u001b[m                pycocoEvalDemo.ipynb utils.py\n",
      "coco_eval.py         \u001b[1m\u001b[36mpycocotools\u001b[m\u001b[m          \u001b[1m\u001b[36mvision\u001b[m\u001b[m\n",
      "coco_utils.py        \u001b[1m\u001b[36mpycocotools.egg-info\u001b[m\u001b[m\n",
      "\u001b[1m\u001b[36mcocoapi\u001b[m\u001b[m              script.py\n",
      "Requirement already satisfied: cython in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (0.29.14)\n",
      "Requirement already satisfied: numpy in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (1.18.1)\n",
      "running build_ext\n",
      "skipping 'pycocotools/_mask.c' Cython extension (up-to-date)\n",
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing pycocotools.egg-info/PKG-INFO\n",
      "writing dependency_links to pycocotools.egg-info/dependency_links.txt\n",
      "writing requirements to pycocotools.egg-info/requires.txt\n",
      "writing top-level names to pycocotools.egg-info/top_level.txt\n",
      "reading manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
      "writing manifest file 'pycocotools.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.macosx-10.9-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.macosx-10.9-x86_64/egg\n",
      "creating build/bdist.macosx-10.9-x86_64/egg/pycocotools\n",
      "copying build/lib.macosx-10.9-x86_64-3.8/pycocotools/coco.py -> build/bdist.macosx-10.9-x86_64/egg/pycocotools\n",
      "copying build/lib.macosx-10.9-x86_64-3.8/pycocotools/mask.py -> build/bdist.macosx-10.9-x86_64/egg/pycocotools\n",
      "copying build/lib.macosx-10.9-x86_64-3.8/pycocotools/__init__.py -> build/bdist.macosx-10.9-x86_64/egg/pycocotools\n",
      "copying build/lib.macosx-10.9-x86_64-3.8/pycocotools/cocoeval.py -> build/bdist.macosx-10.9-x86_64/egg/pycocotools\n",
      "copying build/lib.macosx-10.9-x86_64-3.8/pycocotools/_mask.cpython-38-darwin.so -> build/bdist.macosx-10.9-x86_64/egg/pycocotools\n",
      "byte-compiling build/bdist.macosx-10.9-x86_64/egg/pycocotools/coco.py to coco.cpython-38.pyc\n",
      "byte-compiling build/bdist.macosx-10.9-x86_64/egg/pycocotools/mask.py to mask.cpython-38.pyc\n",
      "byte-compiling build/bdist.macosx-10.9-x86_64/egg/pycocotools/__init__.py to __init__.cpython-38.pyc\n",
      "byte-compiling build/bdist.macosx-10.9-x86_64/egg/pycocotools/cocoeval.py to cocoeval.cpython-38.pyc\n",
      "creating stub loader for pycocotools/_mask.cpython-38-darwin.so\n",
      "byte-compiling build/bdist.macosx-10.9-x86_64/egg/pycocotools/_mask.py to _mask.cpython-38.pyc\n",
      "creating build/bdist.macosx-10.9-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/PKG-INFO -> build/bdist.macosx-10.9-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/SOURCES.txt -> build/bdist.macosx-10.9-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/dependency_links.txt -> build/bdist.macosx-10.9-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/requires.txt -> build/bdist.macosx-10.9-x86_64/egg/EGG-INFO\n",
      "copying pycocotools.egg-info/top_level.txt -> build/bdist.macosx-10.9-x86_64/egg/EGG-INFO\n",
      "writing build/bdist.macosx-10.9-x86_64/egg/EGG-INFO/native_libs.txt\n",
      "zip_safe flag not set; analyzing archive contents...\n",
      "pycocotools.__pycache__._mask.cpython-38: module references __file__\n",
      "creating 'dist/pycocotools-2.0-py3.8-macosx-10.9-x86_64.egg' and adding 'build/bdist.macosx-10.9-x86_64/egg' to it\n",
      "removing 'build/bdist.macosx-10.9-x86_64/egg' (and everything under it)\n",
      "Processing pycocotools-2.0-py3.8-macosx-10.9-x86_64.egg\n",
      "removing '/Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/pycocotools-2.0-py3.8-macosx-10.9-x86_64.egg' (and everything under it)\n",
      "creating /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/pycocotools-2.0-py3.8-macosx-10.9-x86_64.egg\n",
      "Extracting pycocotools-2.0-py3.8-macosx-10.9-x86_64.egg to /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages\n",
      "pycocotools 2.0 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/pycocotools-2.0-py3.8-macosx-10.9-x86_64.egg\n",
      "Processing dependencies for pycocotools==2.0\n",
      "Searching for matplotlib==3.2.0rc3\n",
      "Best match: matplotlib 3.2.0rc3\n",
      "Processing matplotlib-3.2.0rc3-py3.8-macosx-10.9-x86_64.egg\n",
      "matplotlib 3.2.0rc3 is already the active version in easy-install.pth\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/matplotlib-3.2.0rc3-py3.8-macosx-10.9-x86_64.egg\n",
      "Searching for Cython==0.29.14\n",
      "Best match: Cython 0.29.14\n",
      "Adding Cython 0.29.14 to easy-install.pth file\n",
      "Installing cygdb script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "Installing cython script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "Installing cythonize script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages\n",
      "Searching for setuptools==45.1.0.post20200127\n",
      "Best match: setuptools 45.1.0.post20200127\n",
      "Adding setuptools 45.1.0.post20200127 to easy-install.pth file\n",
      "Installing easy_install script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages\n",
      "Searching for python-dateutil==2.8.1\n",
      "Best match: python-dateutil 2.8.1\n",
      "Adding python-dateutil 2.8.1 to easy-install.pth file\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages\n",
      "Searching for pyparsing==2.4.6\n",
      "Best match: pyparsing 2.4.6\n",
      "Processing pyparsing-2.4.6-py3.8.egg\n",
      "pyparsing 2.4.6 is already the active version in easy-install.pth\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/pyparsing-2.4.6-py3.8.egg\n",
      "Searching for numpy==1.18.1\n",
      "Best match: numpy 1.18.1\n",
      "Adding numpy 1.18.1 to easy-install.pth file\n",
      "Installing f2py script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "Installing f2py3 script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "Installing f2py3.8 script to /Users/gkv/miniconda3/envs/azureml/bin\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages\n",
      "Searching for kiwisolver==1.1.0\n",
      "Best match: kiwisolver 1.1.0\n",
      "Processing kiwisolver-1.1.0-py3.8-macosx-10.9-x86_64.egg\n",
      "kiwisolver 1.1.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/kiwisolver-1.1.0-py3.8-macosx-10.9-x86_64.egg\n",
      "Searching for cycler==0.10.0\n",
      "Best match: cycler 0.10.0\n",
      "Processing cycler-0.10.0-py3.8.egg\n",
      "cycler 0.10.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/cycler-0.10.0-py3.8.egg\n",
      "Searching for six==1.14.0\n",
      "Best match: six 1.14.0\n",
      "Adding six 1.14.0 to easy-install.pth file\n",
      "\n",
      "Using /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages\n",
      "Finished processing dependencies for pycocotools==2.0\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#sys.path.insert(0,'./')\n",
    "# Install pycocotools\n",
    "# !git clone https://github.com/cocodataset/cocoapi.git\n",
    "# !ls\n",
    "# %cd cocoapi/PythonAPI\n",
    "# !ls\n",
    "# !pip install cython\n",
    "# !pip install numpy\n",
    "# !python setup.py build_ext install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.85\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning diagnostics collection on. \n"
     ]
    }
   ],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: gopalv-ws\n",
      "Azure region: westus2\n",
      "Subscription id: 15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\n",
      "Resource group: aifxdemo\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n",
      "{'currentNodeCount': 0, 'targetNodeCount': 0, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 0, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2020-02-16T23:53:03.831000+00:00', 'errors': None, 'creationTime': '2020-02-05T18:19:06.976503+00:00', 'modifiedTime': '2020-02-05T18:20:28.120151+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 2, 'nodeIdleTimeBeforeScaleDown': 'PT120S'}, 'vmPriority': 'LowPriority', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./test.zip', <http.client.HTTPMessage at 0x10cdfdaf0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    " \n",
    "url = 'https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip'\n",
    " \n",
    "urllib.request.urlretrieve(url, './test.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "zip = ZipFile(file='./test.zip')\n",
    "zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mAnnotation\u001b[m\u001b[m            \u001b[1m\u001b[36mPedMasks\u001b[m\u001b[m              readme.txt\r\n",
      "\u001b[1m\u001b[36mPNGImages\u001b[m\u001b[m             added-object-list.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls PennFudanPed/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workspaceblobstore AzureBlob gopalvws3790775563 azureml-blobstore-e47496c6-9688-4277-a05b-ceb722514b9d\n"
     ]
    }
   ],
   "source": [
    "# get the default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputExpanded": true,
    "outputHidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading an estimated of 512 files\n",
      "Target already exists. Skipping upload for data/added-object-list.txt\n",
      "Target already exists. Skipping upload for data/readme.txt\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00048_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00049_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00072_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00073_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00001_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00093_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00092_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00076_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00077_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00005_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00004_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00031_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00030_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00042_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00043_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00035_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00034_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00046_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00047_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00084_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00085_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00016_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00017_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00065_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00064_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00012_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00013_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00028_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00029_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00061_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00060_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00055_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00054_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00026_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00027_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00051_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00050_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00022_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00023_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00018_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00019_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00011_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00010_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00083_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00082_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00058_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00059_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00062_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00063_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00015_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00014_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00066_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00067_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00068_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00069_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00052_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00053_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00021_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00020_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00089_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00088_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00056_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00057_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00025_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00024_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00074_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00094_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00095_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00006_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00007_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00071_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00070_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00002_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00003_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00038_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00039_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00036_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00037_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00045_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00044_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00032_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00033_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00008_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00009_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00041_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00040_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00056_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00057_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00025_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00024_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00068_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00069_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00052_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00053_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00021_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00020_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00087_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00086_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00015_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00014_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00066_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00067_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00011_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00010_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00058_mask.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00059_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00062_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00063_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00032_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00033_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00008_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00009_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00041_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00040_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00036_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00037_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00045_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00044_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00071_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00070_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00002_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00003_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00090_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00091_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00038_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00039_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00075_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00074_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00006_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00007_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00035_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00034_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00046_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00047_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00031_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00030_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00078_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00079_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00042_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00043_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00096_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00005_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00004_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00048_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00049_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00072_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00073_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00001_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00051_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00050_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00022_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00023_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00018_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00019_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00055_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00054_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00026_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00027_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00012_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00013_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00028_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00029_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00080_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00081_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00061_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00060_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00016_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/FudanPed00017_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00065_mask.png\n",
      "Target already exists. Skipping upload for data/PedMasks/PennPed00064_mask.png\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00013.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00007.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00014.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00028.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00029.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00015.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00001.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00006.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00012.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00004.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00010.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00038.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00017.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00003.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00002.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00016.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00039.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00011.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00005.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00029.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00001.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00015.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00012.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00006.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00007.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00013.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00014.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00028.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00016.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00002.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00039.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00005.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00011.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00010.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00004.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00038.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00003.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00017.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00070.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00064.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00058.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for data/Annotation/FudanPed00063.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00062.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00059.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00065.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00071.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00067.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00073.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00074.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00060.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00048.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00049.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00061.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00072.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00066.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00089.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00062.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00076.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00059.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00071.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00065.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00064.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00070.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00058.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00077.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00063.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00088.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00049.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00075.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00061.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00066.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00072.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00073.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00067.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00060.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00074.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00048.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00092.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00086.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00051.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00045.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00079.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00042.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00056.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00057.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00043.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00078.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00044.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00050.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00087.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00093.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00085.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00091.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00046.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00052.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00055.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00041.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00069.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00068.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00040.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00054.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00053.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00047.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00090.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00084.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00080.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00094.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00043.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00057.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00050.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00044.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00045.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00051.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00056.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00042.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00095.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00081.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00083.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00068.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00054.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00040.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00047.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00053.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00052.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00046.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00041.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00055.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00069.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00082.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00096.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00032.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00026.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00021.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00035.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00009.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00008.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00034.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00020.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00027.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00033.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00025.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00031.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00019.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00036.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00022.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00023.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00037.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00018.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00030.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00024.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for data/Annotation/PennPed00008.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00020.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00034.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00033.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00027.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00026.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00032.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00035.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00021.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00009.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00037.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00023.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00018.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00024.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00030.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00031.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00025.txt\n",
      "Target already exists. Skipping upload for data/Annotation/FudanPed00019.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00022.txt\n",
      "Target already exists. Skipping upload for data/Annotation/PennPed00036.txt\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00062.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00059.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00071.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00065.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00064.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00070.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00058.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00063.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00049.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00061.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00066.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00072.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00073.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00067.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00060.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00074.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00048.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00070.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00064.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00058.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00063.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00077.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00088.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00089.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00076.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00062.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00059.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00065.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00071.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00067.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00073.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00074.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00060.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00048.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00049.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00061.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00075.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00072.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00066.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00029.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00001.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00015.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00012.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00006.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00007.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00013.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00014.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00028.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00016.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00002.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00039.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00005.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00011.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00010.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00004.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00038.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00003.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00017.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00013.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00007.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00014.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00028.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00029.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00015.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00001.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00006.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00012.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00004.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00010.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00038.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00017.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00003.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00002.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00016.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00039.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00011.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00005.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00008.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00020.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00034.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00033.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00027.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00026.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00032.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00035.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00021.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00009.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00037.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00023.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00018.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target already exists. Skipping upload for data/PNGImages/PennPed00024.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00030.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00031.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00025.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00019.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00022.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00036.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00032.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00026.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00021.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00035.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00009.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00008.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00034.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00020.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00027.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00033.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00025.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00031.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00019.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00036.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00022.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00023.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00037.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00018.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00030.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00024.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00043.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00057.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00078.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00050.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00044.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00093.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00087.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00086.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00092.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00045.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00051.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00079.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00056.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00042.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00068.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00054.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00040.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00047.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00053.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00084.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00090.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00091.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00085.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00052.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00046.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00041.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00055.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00069.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00051.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00045.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00042.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00056.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00081.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00095.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00094.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00080.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00057.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00043.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00044.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00050.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00046.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00052.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00055.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00041.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00069.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00096.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00082.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00083.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00068.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00040.png\n",
      "Target already exists. Skipping upload for data/PNGImages/PennPed00054.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00053.png\n",
      "Target already exists. Skipping upload for data/PNGImages/FudanPed00047.png\n",
      "Uploaded 0 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_26d2302af8f64b43a3c0bc9a249bffab"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.upload('./PennFudanPed', target_path='data', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'data')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"1cd2dd50-ae6d-44f9-81e2-f0044c862efe\",\n",
       "    \"name\": \"penn_ds\",\n",
       "    \"version\": 1,\n",
       "    \"description\": \"Penn Fudan pedestrian data\",\n",
       "    \"workspace\": \"Workspace.create(name='gopalv-ws', subscription_id='15ae9cb6-95c1-483d-a0e3-b1a1a3b06324', resource_group='aifxdemo')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "# create a FileDataset pointing to files in 'animals' folder and its subfolders recursively\n",
    "datastore_paths = [(ds, 'data')]\n",
    "penn_ds = Dataset.File.from_files(path=datastore_paths)\n",
    "penn_ds.register(workspace=ws,\n",
    "                 name='penn_ds',\n",
    "                 description='Penn Fudan pedestrian data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "outputHidden": true
   },
   "outputs": [],
   "source": [
    "# dataset_name = 'penn_ds'\n",
    "\n",
    "# # Get a dataset by name\n",
    "# penn1_ds = Dataset.get_by_name(workspace=ws, name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azureml.data.dataset_consumption_config.DatasetConsumptionConfig at 0x122723130>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "# import glob\n",
    "\n",
    "\n",
    "# with penn_ds.mount() as mount_context:\n",
    "#     # list top level mounted files and folders in the dataset\n",
    "#     imgs_path =(os.path.join(mount_context.mount_point, 'PNGImages'))\n",
    "#     print(f'imgs_path is {imgs_path}')\n",
    "#     print(list(sorted(os.listdir(imgs_path))))\n",
    "#     Image.open(os.path.join(mount_context.mount_point, 'PNGImages/PennPed00036.png'))\n",
    "penn1_ds.as_named_input('test').as_mount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.PennFudanDataset at 0x122a2f040>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PennFudanDataset(penn_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE     README.rst  hubconf.py  setup.cfg   \u001b[1m\u001b[36mtest\u001b[m\u001b[m        tox.ini\n",
      "MANIFEST.in \u001b[1m\u001b[36mdocs\u001b[m\u001b[m        \u001b[1m\u001b[36mreferences\u001b[m\u001b[m  setup.py    \u001b[1m\u001b[36mtorchvision\u001b[m\u001b[m\n",
      "/Users/gkv/MS/pytorch-object\n",
      "\u001b[1m\u001b[36mPennFudanPed\u001b[m\u001b[m                     \u001b[1m\u001b[36mpytorch-peds\u001b[m\u001b[m\n",
      "PyTorch-object-detection-1.ipynb script.py\n",
      "coco_eval.py                     test.zip\n",
      "coco_utils.py                    transforms.py\n",
      "config.json                      utils.py\n",
      "engine.py                        \u001b[1m\u001b[36mvision\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "%cd ..\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vision' already exists and is not an empty directory.\n",
      "error: pathspec 'v0.3.0' did not match any file(s) known to git\n",
      "/Users/gkv/MS/pytorch-object/vision\n",
      "/Users/gkv/MS/pytorch-object\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pytorch/vision.git\n",
    "\n",
    "!git checkout v0.3.0\n",
    "\n",
    "%cd vision\n",
    "!cp references/detection/utils.py ../\n",
    "!cp references/detection/transforms.py ../\n",
    "!cp references/detection/coco_eval.py ../\n",
    "!cp references/detection/engine.py ../\n",
    "!cp references/detection/coco_utils.py ../\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "os.listdir()\n",
    "\n",
    "files_to_copy = ['utils', 'transforms', 'coco_eval', 'engine', 'coco_utils']\n",
    "for file in files_to_copy:\n",
    "    shutil.copy('./vision/references/detection/'+ file + '.py', project_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'pytorch-peds'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "my_env = Environment(name='maskr-docker')\n",
    "my_env.docker.enabled = True\n",
    "with open(\"Dockerfile\", \"r\") as f:\n",
    "    dockerfile_contents_of_your_base_image=f.read()\n",
    "my_env.docker.base_dockerfile=dockerfile_contents_of_your_base_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(Experiment: pytorch-peds,\n",
      "Id: pytorch-peds_1582495865_7b77bc3b,\n",
      "Type: azureml.scriptrun,\n",
      "Status: Starting)\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "# follow pattern from here: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-use-environments#use-environments-for-training\n",
    "\n",
    "# Add training script to run config\n",
    "runconfig = ScriptRunConfig(source_directory=\".\", script=\"script.py\")\n",
    "\n",
    "# Attach compute target to run config\n",
    "runconfig.run_config.target = \"local\"\n",
    "\n",
    "my_env.docker.base_image = None\n",
    "# Attach environment to run config\n",
    "runconfig.run_config.environment = my_env\n",
    "\n",
    "# Submit run \n",
    "run = experiment.submit(runconfig)\n",
    "print(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'runId': 'pytorch-peds_1582495865_7b77bc3b', 'target': 'local', 'status': 'Failed', 'startTimeUtc': '2020-02-23T22:11:07.001013Z', 'endTimeUtc': '2020-02-23T22:11:08.871966Z', 'error': {'error': {'code': 'UserError', 'message': 'Failed to build the docker image with exit code 1. Please check azureml-logs/60_control_log.txt for more details.', 'details': []}, 'time': '0001-01-01T00:00:00.000Z'}, 'warnings': [{'source': 'SecondaryError', 'message': '{\\n  \"error\": {\\n    \"code\": \"ServiceError\",\\n    \"message\": \"SystemExit: 1\",\\n    \"detailsUri\": null,\\n    \"target\": null,\\n    \"details\": [],\\n    \"innerError\": null,\\n    \"debugInfo\": null\\n  },\\n  \"correlation\": null,\\n  \"environment\": null,\\n  \"location\": null,\\n  \"time\": \"0001-01-01T00:00:00+00:00\"\\n}'}], 'properties': {'_azureml.ComputeTargetType': 'local', 'ContentSnapshotId': '590e058a-1d25-4c4f-9120-acc067bf38c5', 'azureml.git.repository_uri': 'https://github.com/gvashishtha/pytorch-object.git', 'mlflow.source.git.repoURL': 'https://github.com/gvashishtha/pytorch-object.git', 'azureml.git.branch': 'docker', 'mlflow.source.git.branch': 'docker', 'azureml.git.commit': '7c00f24c4d62ca70cea0161c8e99fcaff13edbd8', 'mlflow.source.git.commit': '7c00f24c4d62ca70cea0161c8e99fcaff13edbd8', 'azureml.git.dirty': 'True'}, 'inputDatasets': [], 'runDefinition': {'script': 'script.py', 'useAbsolutePath': False, 'arguments': [], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'local', 'dataReferences': {}, 'data': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'environment': {'name': 'maskr-docker', 'version': 'Autosave_2020-02-23T22:11:06Z_168aa0dc', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}], 'name': 'azureml_1b417bb747e35859ebf611fb43071e9c'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': None, 'baseDockerfile': '# Copyright (c) Microsoft Corporation. All rights reserved.\\n# Licensed under the MIT License.\\n\\nFROM mcr.microsoft.com/azureml/o16n-base/python-assets@sha256:20a8b655a3e5b9b0db8ddf70d03d048a7cf49e569c4f0382198b1ee77631a6ad AS inferencing-assets\\n\\n# Tag: cuda:10.1-cudnn7-devel-ubuntu18.04\\n# Env: CUDA_VERSION=10.1.243\\n# Env: CUDA_PKG_VERSION=10-1=10.1.243-1\\n# Env: NCCL_VERSION=2.4.8\\n# Env: CUDNN_VERSION=7.6.3.30\\n# Env: NVIDIA_VISIBLE_DEVICES=all\\n# Env: NVIDIA_DRIVER_CAPABILITIES=compute,utility\\n# Env: NVIDIA_REQUIRE_CUDA=cuda>=10.1 brand=tesla,driver>=384,driver<385 brand=tesla,driver>=410,driver<411\\n# Label: com.nvidia.cuda.version=10.1.243\\n# Label: com.nvidia.cudnn.version=7.6.3.30\\n# Label: com.nvidia.volumes.needed=nvidia_driver\\n# Ubuntu 18.04\\nFROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04\\n\\nUSER root:root\\n\\nENV com.nvidia.cuda.version $CUDA_VERSION\\nENV com.nvidia.volumes.needed nvidia_driver\\nENV LANG=C.UTF-8 LC_ALL=C.UTF-8\\nENV DEBIAN_FRONTEND noninteractive\\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\\nENV NCCL_DEBUG=INFO\\nENV HOROVOD_GPU_ALLREDUCE=NCCL\\n\\n# Install Common Dependencies\\nRUN apt-get update && \\\\\\n    apt-get install -y --no-install-recommends \\\\\\n    # SSH and RDMA\\n    libmlx4-1 \\\\\\n    libmlx5-1 \\\\\\n    librdmacm1 \\\\\\n    libibverbs1 \\\\\\n    libmthca1 \\\\\\n    libdapl2 \\\\\\n    dapl2-utils \\\\\\n    openssh-client \\\\\\n    openssh-server \\\\\\n    iproute2 && \\\\\\n    # Others\\n    apt-get install -y \\\\\\n    build-essential \\\\\\n    bzip2=1.0.6-8.1ubuntu0.2 \\\\\\n    libbz2-1.0=1.0.6-8.1ubuntu0.2 \\\\\\n    systemd \\\\\\n    git=1:2.17.1-1ubuntu0.4 \\\\\\n    wget \\\\\\n    cpio \\\\\\n    libsm6 \\\\\\n    libxext6 \\\\\\n    libxrender-dev \\\\\\n    fuse && \\\\\\n    apt-get clean -y && \\\\\\n    rm -rf /var/lib/apt/lists/*\\n\\n# Inference\\n# Copy logging utilities, nginx and rsyslog configuration files, IOT server binary, etc.\\nCOPY --from=inferencing-assets /artifacts /var/\\nRUN /var/requirements/install_system_requirements.sh && \\\\\\n    cp /var/configuration/rsyslog.conf /etc/rsyslog.conf && \\\\\\n    cp /var/configuration/nginx.conf /etc/nginx/sites-available/app && \\\\\\n    ln -s /etc/nginx/sites-available/app /etc/nginx/sites-enabled/app && \\\\\\n    rm -f /etc/nginx/sites-enabled/default\\nENV SVDIR=/var/runit\\nENV WORKER_TIMEOUT=300\\nEXPOSE 5001 8883 8888\\n\\n# Conda Environment\\nENV MINICONDA_VERSION 4.5.11\\nENV PATH /opt/miniconda/bin:$PATH\\nRUN wget -qO /tmp/miniconda.sh https://repo.continuum.io/miniconda/Miniconda3-${MINICONDA_VERSION}-Linux-x86_64.sh && \\\\\\n    bash /tmp/miniconda.sh -bf -p /opt/miniconda && \\\\\\n    conda clean -ay && \\\\\\n    rm -rf /opt/miniconda/pkgs && \\\\\\n    rm /tmp/miniconda.sh && \\\\\\n    find / -type d -name __pycache__ | xargs rm -rf\\n\\n# Open-MPI installation\\nENV OPENMPI_VERSION 3.1.2\\nRUN mkdir /tmp/openmpi && \\\\\\n    cd /tmp/openmpi && \\\\\\n    wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-${OPENMPI_VERSION}.tar.gz && \\\\\\n    tar zxf openmpi-${OPENMPI_VERSION}.tar.gz && \\\\\\n    cd openmpi-${OPENMPI_VERSION} && \\\\\\n    ./configure --enable-orterun-prefix-by-default && \\\\\\n    make -j $(nproc) all && \\\\\\n    make install && \\\\\\n    ldconfig && \\\\\\n    rm -rf /tmp/openmpi\\n', 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': True, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}}, 'logFiles': {'azureml-logs/60_control_log.txt': 'https://gopalvws3790775563.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-peds_1582495865_7b77bc3b/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=eF6bxlyirpVageBE9DYcPBgyDXonEKTLCRBXIJybSQg%3D&st=2020-02-23T22%3A01%3A12Z&se=2020-02-24T06%3A11%3A12Z&sp=r'}}\n"
     ]
    }
   ],
   "source": [
    "# to get more details of your run\n",
    "print(run.get_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1b8749f0c244fcb94f69a6083446ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/aml.mini.widget.v1": "{\"status\": \"Failed\", \"workbench_run_details_uri\": \"https://ml.azure.com/experiments/pytorch-peds/runs/pytorch-peds_1582495865_7b77bc3b?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/aifxdemo/workspaces/gopalv-ws\", \"run_id\": \"pytorch-peds_1582495865_7b77bc3b\", \"run_properties\": {\"run_id\": \"pytorch-peds_1582495865_7b77bc3b\", \"created_utc\": \"2020-02-23T22:11:07.001013Z\", \"properties\": {\"_azureml.ComputeTargetType\": \"local\", \"ContentSnapshotId\": \"590e058a-1d25-4c4f-9120-acc067bf38c5\", \"azureml.git.repository_uri\": \"https://github.com/gvashishtha/pytorch-object.git\", \"mlflow.source.git.repoURL\": \"https://github.com/gvashishtha/pytorch-object.git\", \"azureml.git.branch\": \"docker\", \"mlflow.source.git.branch\": \"docker\", \"azureml.git.commit\": \"7c00f24c4d62ca70cea0161c8e99fcaff13edbd8\", \"mlflow.source.git.commit\": \"7c00f24c4d62ca70cea0161c8e99fcaff13edbd8\", \"azureml.git.dirty\": \"True\"}, \"tags\": {}, \"script_name\": null, \"arguments\": null, \"end_time_utc\": \"2020-02-23T22:11:08.871966Z\", \"status\": \"Failed\", \"log_files\": {\"azureml-logs/60_control_log.txt\": \"https://gopalvws3790775563.blob.core.windows.net/azureml/ExperimentRun/dcid.pytorch-peds_1582495865_7b77bc3b/azureml-logs/60_control_log.txt?sv=2019-02-02&sr=b&sig=fF2JObp6uY4BbqEPBI8bkgvlVu9LI5xdeXfXv2KQEN4%3D&st=2020-02-23T22%3A01%3A18Z&se=2020-02-24T06%3A11%3A18Z&sp=r\"}, \"log_groups\": [[\"azureml-logs/60_control_log.txt\"]], \"run_duration\": \"0:00:01\"}, \"child_runs\": [], \"children_metrics\": {}, \"run_metrics\": [], \"run_logs\": \"Streaming log file azureml-logs/60_control_log.txt\\nStarting the daemon thread to refresh tokens in background for process with pid = 38682\\nFileNotFoundError(2, \\\"No such file or directory: 'docker'\\\")\\n\\nDocker was not found on the target, check that it is installed and on the path.\\n\\nLogging error in history service: SystemExit: 1\\n\\nUploading control log...\\n\\nError occurred: Failed to build the docker image with exit code 1. Please check azureml-logs/60_control_log.txt for more details.\\n\", \"graph\": {}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": true, \"log_level\": \"INFO\", \"sdk_version\": \"1.0.85\"}, \"loading\": false}"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: pytorch-peds_1582495865_7b77bc3b\n",
      "Web View: https://ml.azure.com/experiments/pytorch-peds/runs/pytorch-peds_1582495865_7b77bc3b?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/aifxdemo/workspaces/gopalv-ws\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: pytorch-peds_1582495865_7b77bc3b\n",
      "Web View: https://ml.azure.com/experiments/pytorch-peds/runs/pytorch-peds_1582495865_7b77bc3b?wsid=/subscriptions/15ae9cb6-95c1-483d-a0e3-b1a1a3b06324/resourcegroups/aifxdemo/workspaces/gopalv-ws\n",
      "\n",
      "Warnings:\n",
      "{\n",
      "  \"error\": {\n",
      "    \"code\": \"ServiceError\",\n",
      "    \"message\": \"SystemExit: 1\",\n",
      "    \"detailsUri\": null,\n",
      "    \"target\": null,\n",
      "    \"details\": [],\n",
      "    \"innerError\": null,\n",
      "    \"debugInfo\": null\n",
      "  },\n",
      "  \"correlation\": null,\n",
      "  \"environment\": null,\n",
      "  \"location\": null,\n",
      "  \"time\": \"0001-01-01T00:00:00+00:00\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "ActivityFailedException",
     "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Failed to build the docker image with exit code 1. Please check azureml-logs/60_control_log.txt for more details.\",\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Failed to build the docker image with exit code 1. Please check azureml-logs/60_control_log.txt for more details.\\\",\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-cc0ab15b5cf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mRunDetails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.8/site-packages/azureml/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m                 self._stream_run_output(\n\u001b[0m\u001b[1;32m    655\u001b[0m                     \u001b[0mfile_handle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0mwait_post_processing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_post_processing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/azureml/lib/python3.8/site-packages/azureml/core/run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[0;34m(self, file_handle, wait_post_processing, raise_on_error)\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mActivityFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"Failed to build the docker image with exit code 1. Please check azureml-logs/60_control_log.txt for more details.\",\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"Failed to build the docker image with exit code 1. Please check azureml-logs/60_control_log.txt for more details.\\\",\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - framework_version is not specified, defaulting to version 1.3.\n",
      "WARNING - You have specified to install packages in your run. Note that you have overridden Azure ML's installation of the following packages: ['torchvision']. We cannot guarantee image build will succeed.\n"
     ]
    }
   ],
   "source": [
    "from azureml.train.dnn import PyTorch\n",
    "\n",
    "script_params = {\n",
    "    '--num_epochs': 10,\n",
    "    '--output_dir': './outputs'\n",
    "}\n",
    "\n",
    "estimator = PyTorch(source_directory=project_folder, \n",
    "                    script_params=script_params,\n",
    "                    compute_target=compute_target,\n",
    "                    entry_script='script.py',\n",
    "                    use_gpu=True,\n",
    "                    pip_packages=['torchvision>=0.5.0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transforms=None):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        with self.dataset.mount() as mount_context:\n",
    "            self.imgs = list(sorted(os.listdir(os.path.join(mount_context.mount_point, \"PNGImages\"))))\n",
    "            self.masks = list(sorted(os.listdir(os.path.join(mount_context.mount_point, \"PedMasks\"))))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        with self.dataset.mount() as mount_context:\n",
    "            img_path = os.path.join(mount_context.mount_point, \"PNGImages\", self.imgs[idx])\n",
    "            mask_path = os.path.join(mount_context.mount_point, \"PedMasks\", self.masks[idx])\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            # note that we haven't converted the mask to RGB,\n",
    "            # because each color corresponds to a different instance\n",
    "            # with 0 being background\n",
    "            mask = Image.open(mask_path)\n",
    "\n",
    "            mask = np.array(mask)\n",
    "            # instances are encoded as different colors\n",
    "            obj_ids = np.unique(mask)\n",
    "            # first id is the background, so remove it\n",
    "            obj_ids = obj_ids[1:]\n",
    "\n",
    "            # split the color-encoded mask into a set\n",
    "            # of binary masks\n",
    "            masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "            # get bounding box coordinates for each mask\n",
    "            num_objs = len(obj_ids)\n",
    "            boxes = []\n",
    "            for i in range(num_objs):\n",
    "                pos = np.where(masks[i])\n",
    "                xmin = np.min(pos[1])\n",
    "                xmax = np.max(pos[1])\n",
    "                ymin = np.min(pos[0])\n",
    "                ymax = np.max(pos[0])\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            # there is only one class\n",
    "            labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "            masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "            image_id = torch.tensor([idx])\n",
    "            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "            # suppose all instances are not crowd\n",
    "            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "            target = {}\n",
    "            target[\"boxes\"] = boxes\n",
    "            target[\"labels\"] = labels\n",
    "            target[\"masks\"] = masks\n",
    "            target[\"image_id\"] = image_id\n",
    "            target[\"area\"] = area\n",
    "            target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "            if self.transforms is not None:\n",
    "                img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torchvision in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (0.5.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (from torchvision) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (from torchvision) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied, skipping upgrade: torch==1.4.0 in /Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages (from torchvision) (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "      \n",
    "def get_instance_segmentation_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n",
      "\n",
      "Please wait a moment while I gather a list of all available modules...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gkv/miniconda3/envs/azureml/lib/python3.8/site-packages/IPython/kernel/__init__.py:12: ShimWarning: The `IPython.kernel` package has been deprecated since IPython 4.0.You should import from ipykernel or jupyter_client instead.\n",
      "  warn(\"The `IPython.kernel` package has been deprecated since IPython 4.0.\"\n",
      "/Users/gkv/miniconda3/envs/azureml/lib/python3.8/pkgutil.py:107: VisibleDeprecationWarning: zmq.eventloop.minitornado is deprecated in pyzmq 14.0 and will be removed.\n",
      "    Install tornado itself to use zmq with the tornado IOLoop.\n",
      "    \n",
      "  yield from walk_packages(path, info.name+'.', onerror)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cython              asynchat            inspect             random\n",
      "IPython             asyncio             io                  re\n",
      "OpenSSL             asyncore            ipaddress           readline\n",
      "PIL                 atexit              ipykernel           reprlib\n",
      "PyQt5               attr                ipykernel_launcher  requests\n",
      "__future__          audioop             ipython_genutils    requests_oauthlib\n",
      "_abc                automl              ipywidgets          resource\n",
      "_ast                autoreload          isodate             rlcompleter\n",
      "_asyncio            azureml             itertools           rmagic\n",
      "_bisect             backcall            jaraco              runpy\n",
      "_blake2             backports           jedi                sched\n",
      "_bootlocale         base64              jeepney             secrets\n",
      "_bz2                bdb                 jinja2              secretstorage\n",
      "_cffi_backend       binascii            jmespath            select\n",
      "_codecs             binhex              json                selectors\n",
      "_codecs_cn          bisect              jsonpickle          send2trash\n",
      "_codecs_hk          bleach              jsonschema          setup\n",
      "_codecs_iso2022     builtins            jupyter             setuptools\n",
      "_codecs_jp          bz2                 jupyter_client      shelve\n",
      "_codecs_kr          cProfile            jupyter_console     shlex\n",
      "_codecs_tw          caffe2              jupyter_core        shutil\n",
      "_collections        calendar            jwt                 signal\n",
      "_collections_abc    certifi             keyword             sip\n",
      "_compat_pickle      cffi                kiwisolver          sipconfig\n",
      "_compression        cgi                 lib2to3             sipdistutils\n",
      "_contextvars        cgitb               linecache           site\n",
      "_crypt              chardet             locale              six\n",
      "_csv                chunk               logging             smtpd\n",
      "_ctypes             cloudpickle         lzma                smtplib\n",
      "_ctypes_test        cmath               mailbox             sndhdr\n",
      "_curses             cmd                 mailcap             socket\n",
      "_curses_panel       coco_eval           markupsafe          socketserver\n",
      "_datetime           coco_utils          marshal             sqlite3\n",
      "_dbm                code                math                sre_compile\n",
      "_decimal            codecs              matplotlib          sre_constants\n",
      "_dummy_thread       codeop              mccabe              sre_parse\n",
      "_elementtree        collections         mimetypes           ssl\n",
      "_functools          colorsys            mistune             stat\n",
      "_hashlib            compileall          mkl                 statistics\n",
      "_heapq              concurrent          mkl_fft             storemagic\n",
      "_imp                configparser        mmap                string\n",
      "_io                 contextlib          modulefinder        stringprep\n",
      "_json               contextlib2         more_itertools      struct\n",
      "_locale             contextvars         mpl_toolkits        subprocess\n",
      "_lsprof             copy                msrest              sunau\n",
      "_lzma               copyreg             msrestazure         symbol\n",
      "_markupbase         crypt               multiprocessing     sympyprinting\n",
      "_md5                cryptography        nbconvert           symtable\n",
      "_multibytecodec     csv                 nbformat            sys\n",
      "_multiprocessing    ctypes              ndg                 sysconfig\n",
      "_opcode             curses              netrc               syslog\n",
      "_operator           cycler              nis                 tabnanny\n",
      "_osx_support        cython              nntplib             tarfile\n",
      "_pickle             cythonmagic         notebook            telnetlib\n",
      "_posixshmem         dataclasses         ntpath              tempfile\n",
      "_posixsubprocess    datetime            nturl2path          terminado\n",
      "_py_abc             dateutil            numbers             termios\n",
      "_pydecimal          dbm                 numpy               test\n",
      "_pyio               decimal             oauthlib            testpath\n",
      "_pyrsistent_version decorator           opcode              tests\n",
      "_queue              defusedxml          operator            textwrap\n",
      "_random             difflib             optparse            this\n",
      "_ruamel_yaml        dis                 os                  threading\n",
      "_scproxy            distro              pandocfilters       time\n",
      "_sha1               distutils           parser              timeit\n",
      "_sha256             docker              parso               tkinter\n",
      "_sha3               doctest             pathlib             token\n",
      "_sha512             dotnetcore2         pathspec            tokenize\n",
      "_signal             dummy_threading     pdb                 torch\n",
      "_sitebuiltins       easy_install        pexpect             torchvision\n",
      "_socket             email               pickle              tornado\n",
      "_sqlite3            encodings           pickleshare         trace\n",
      "_sre                engine              pickletools         traceback\n",
      "_ssl                ensurepip           pip                 tracemalloc\n",
      "_stat               entrypoints         pipes               traitlets\n",
      "_statistics         enum                pkg_resources       transforms\n",
      "_string             errno               pkgutil             tty\n",
      "_strptime           faulthandler        platform            turtle\n",
      "_struct             fcntl               plistlib            turtledemo\n",
      "_symtable           filecmp             poplib              types\n",
      "_sysconfigdata__darwin_darwin fileinput           posix               typing\n",
      "_sysconfigdata_aarch64_conda_cos7_linux_gnu flake8              posixpath           unicodedata\n",
      "_sysconfigdata_i686_conda_cos6_linux_gnu fnmatch             pprint              unittest\n",
      "_sysconfigdata_powerpc64le_conda_cos7_linux_gnu formatter           profile             urllib\n",
      "_sysconfigdata_x86_64_apple_darwin13_4_0 fractions           prometheus_client   urllib3\n",
      "_sysconfigdata_x86_64_conda_cos6_linux_gnu ftplib              prompt_toolkit      utils\n",
      "_testbuffer         functools           pstats              uu\n",
      "_testcapi           fuse                pty                 uuid\n",
      "_testimportmultiple gc                  ptyprocess          venv\n",
      "_testinternalcapi   genericpath         pvectorc            warnings\n",
      "_testmultiphase     getopt              pwd                 wave\n",
      "_thread             getpass             py_compile          wcwidth\n",
      "_threading_local    gettext             pyasn1              weakref\n",
      "_tkinter            glob                pyclbr              webbrowser\n",
      "_tracemalloc        grp                 pycocotools         webencodings\n",
      "_uuid               gzip                pycodestyle         websocket\n",
      "_warnings           hashlib             pycparser           wheel\n",
      "_weakref            heapq               pydoc               widgetsnbextension\n",
      "_weakrefset         hmac                pydoc_data          wsgiref\n",
      "_xxsubinterpreters  html                pyexpat             xdrlib\n",
      "_xxtestfuzz         http                pyflakes            xml\n",
      "abc                 hubconf             pygments            xmlrpc\n",
      "adal                idlelib             pylab               xxlimited\n",
      "aifc                idna                pyparsing           xxsubtype\n",
      "antigravity         imaplib             pyrsistent          zipapp\n",
      "applicationinsights imghdr              pytz                zipfile\n",
      "appnope             imp                 pyximport           zipimport\n",
      "argparse            importlib           qtconsole           zipp\n",
      "array               importlib_metadata  queue               zlib\n",
      "ast                 inflect             quopri              zmq\n",
      "\n",
      "Enter any module name to get more help.  Or, type \"modules spam\" to search\n",
      "for modules whose name or summary contain the string \"spam\".\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gkv/miniconda3/envs/azureml/lib/python3.8/pkgutil.py:92: MatplotlibDeprecationWarning: \n",
      "The mpl_toolkits.axes_grid module was deprecated in Matplotlib 2.1 and will be removed two minor releases later. Use mpl_toolkits.axes_grid1 and mpl_toolkits.axisartist, which provide the same functionality instead.\n",
      "  __import__(info.name)\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "help(\"modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# from engine import train_one_epoch, evaluate\n",
    "# import utils\n",
    "# import transforms as T\n",
    "\n",
    "# def get_transform(train):\n",
    "#     transforms = []\n",
    "#     # converts the image, a PIL image, into a PyTorch Tensor\n",
    "#     transforms.append(T.ToTensor())\n",
    "#     if train:\n",
    "#         # during training, randomly flip the training images\n",
    "#         # and ground-truth for data augmentation\n",
    "#         transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "#     return T.Compose(transforms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# # use our dataset and defined transformations\n",
    "# dataset = PennFudanDataset(penn_ds, get_transform(train=True))\n",
    "# dataset_test = PennFudanDataset(penn_ds, get_transform(train=False))\n",
    "\n",
    "# # split the dataset in train and test set\n",
    "# torch.manual_seed(1)\n",
    "# indices = torch.randperm(len(dataset)).tolist()\n",
    "# dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
    "\n",
    "# # define training and validation data loaders\n",
    "# data_loader = torch.utils.data.DataLoader(\n",
    "#     dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "#     collate_fn=utils.collate_fn)\n",
    "\n",
    "# data_loader_test = torch.utils.data.DataLoader(\n",
    "#     dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "#     collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# # our dataset has two classes only - background and person\n",
    "# num_classes = 2\n",
    "\n",
    "# # get the model using our helper function\n",
    "# model = get_instance_segmentation_model(num_classes)\n",
    "# # move model to the right device\n",
    "# model.to(device)\n",
    "\n",
    "# # construct an optimizer\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "#                             momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# # and a learning rate scheduler which decreases the learning rate by\n",
    "# # 10x every 3 epochs\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "#                                                step_size=3,\n",
    "#                                                gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_one_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c750cf7ed566>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_one_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # train for one epoch, printing every 10 iterations\n",
    "#     train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "#     # update the learning rate\n",
    "#     lr_scheduler.step()\n",
    "#     # evaluate on the test dataset\n",
    "#     evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pick one image from the test set\n",
    "# img, _ = dataset_test[0]\n",
    "# # put the model in evaluation mode\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "azureml",
   "language": "python",
   "name": "azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
